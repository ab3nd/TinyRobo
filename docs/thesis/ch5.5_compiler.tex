% !TeX root = ams_thesis.tex
\chapter{Interface Implementation}
\thispagestyle{fancy}

\section{Gesture Recognition}

The user interface for the control system uses the Kivy window management library. 
The UI uses ROS for its infrastructure, with the various components implemented as ROS nodes and communicating using ROS messages.
The user interface layer receives images from the robot arena overhead camera, and displays them, appropriately overlaid with buttons, to the user. 
The topmost layer receives user interactions in the form of contacts, and converts them to ROS touch events. The multitouch device emits a contact points that contain their location on the screen in cartesian coordinates, \todo{what other metadata is present at this level}
Kivy's multitouch device handling adds some other event information, including whether the contact is a double or triple tap, when the touch was last updated, and whether the event has ended (the finger has been lifted) \todo{Be sure about which data comes from where}
Button presses are also converted into ROS button messages\todo{maybe have the messages documented} which contain the time of the button press and the name of the button which was pressed. 

The user interface layer publishes touches, which correspond to single points of contact in both time and space as observed by Kivy. 
In order to collect points into strokes, a degree of cleaning of the input is needed. 
The primary source of error in strokes is stick-slip motion of the user's finger on the screen. 
When the user's finger slips, sometimes the screen regards the finger as having left the surface and returned quickly, breaking the stroke into two separate gestures. 
The ROS node destutter.py maintains a dictionary of lists of touches in the order that they arrived, indexed in the dictionary by the touch ID assigned by the user interface. 
As each stroke ends, it is compared to the other ended strokes. 
If the space in time and distance between the end of the recently completed stroke and the beginning of another stroke is small enough, the two strokes are merged into a single stroke. 
The parameters for the space and time thresholds for merging were set empirically, by intentionally causing stuttering contact with the screen and examining the reported time and distance between the beginnings and ends of contacts. 
At present, if the beginning and end of successive strokes overlap by less than 0.01 second, or are less than 0.15 second apart, they are considered candidates for merging. 
To be merged, candidates must also end and begin no more than 80 pixels apart. 
This value is approximately the width of a fingertip on the 3M screen, and is greater than the observed distance between stuttering strokes. 

Because a currently-ended stroke might be merged with a stroke that hasn't begun yet (that is to say, the user's finger might be in the air during the stutter of a stick-slip movement), strokes that have ended are not published unless there is no in-progress stroke that they could possibly be merged with. 
Every 0.2 seconds, the stroke cleaning node checks all the ended strokes against all the in-progress strokes. 
If the ended stroke's endpoint is closer than the distance threshold from the beginning of all of the active strokes, it could be merged with the active stroke. 
If the ended stroke is close enough to an active stroke's beginning to be considered, the time threshold is checked to determine if they could possibly be merged. 
If there is no active stroke that the current stroke could possibly be merged with, once that active stroke ends, then the current stroke could be published.
If the current stroke's end time is greater than the merge gap threshold in the past, then it is impossible for a new stroke to begin in time to be merged with the current stroke. 
Since there are then no active strokes it could be merged with, and it is impossible for a stroke to begin in time to merge with it, the stroke is then published. 

Strokes are recognized as gestures by a set of separate recognition modules. 
The initial design concept had been to have successive layers of recognition modules add more and more abstract information to the gestures, and arbitrate cases of ambiguity in gestures, but as the selected gesture set was intended to minimize ambiguity, this arbitration lead to additional complexity without any real gain in functionality or ability. 
It should also not be assumed that these gesture recognizers are the best method to perform the task, but they were sufficient during development, and investing significant time in improving them was beyond the scope of this project. 

Gestures are classified into taps, ]lines, arcs, or closed shapes by the angle of the beginning and end of the gesture around the centroid. 
If a gesture has fewer than 10 points or is fewer than 10 pixels across, it is classified as a tap. 
This classification is used because the user's finger distorts as it presses on the screen, and so can be registered as motion over a very small distance, even though the user was not dragging their finger. 
The thresholds for the angle around the centroid were \textless 1 radian to be classified as a circle, \textless 2.5 radians to be classified as an arc, and 2.5 radians or greater to be classified as a line. 
This sort of recognizer has problems with, for example, a circle drawn with a very tight scribble of points at one end. Placing the centroid in the center of the bounding box of the shape would result in it being recognized, correctly, as a circle, but the actual centroid as calculated from the points would be placed close to the dense knot of points at one end of the line, thus skewing the angle around the centroid. 

Gestures are then compared to information about the location of robots relative to the gesture on the screen to determine the meaning of the gestures. 
The gesture detectors are lasso \todo{end this with a full list}

The lasso gesture detector receives gestures from the gesture classifier and from the AprilTag detection node. 
If the gesture starts on a robot, it is considered a drag, where the user puts their finger on one robot, and drags from that robot to another location. 
If the gesture is not a drag, but is a closed shape, and there are robots located inside it, then it is a lasso selection. 
The lasso select gesture recognizer initially attempted to fit an oval to the points of the closed shape, but this approach was rejected because it does not work well if the user attempts to draw a concave polygon, such as a banana shape, in order to select some robots and not select others from a group of robots. 
Instead of using a fitted oval, the closed shape is treated as a polygon, and all robots inside the polygon are considered selected. 
If there are no robots, it is treated as a path, the generic classification for gestures with no other classification. 
If the gesture is not a drag, but is a line, it could be a box selection or a path. 
A box selection is a line whose bounding box includes at least one corner of an AprilTag. 
This requirement for inclusion, rather than including only AprilTags with their centers, or 3/4 of their points inside the bounding box, was chosen because the user survey on inclusion in selections that intersected the robot indicated that erring on the side of including partially-selected robots was preferable. \todo{flowchart this, it's kind of wordy.}

Tap gestures are handled separately from strokes. 
If a tap gesture is on a robot, it is treated as selection of that robot. 
Taps that are not on a robot are treated as potential waypoints. 
For the purposes of making this distinction, a tap located within 80px of a robot is considered on the robot. 
The value of 80 pixels was chosen because it is the approximate width of a fingertip on the interface device, and so should be adjusted for devices of different resolutions. 
Alternatively, the mapping of pixels to real-world dimensions provided by the AprilTags could be used to calculate a conversion factor between the size of the screen and objects displayed on it in pixels and real-world meters, and the distance specified in terms of that conversion factor. 
Doubletap is the end-of-command gesture, and so is not treated as a waypoint or as a robot selection gesture. 

The gestures are then passed to the robot program generator, which adds them to a stack as they arrive. 
When the user terminates the gesture by either issuing an explict end-of-gesture double-tap, or by issuing a new selection gesture, which ends the previous command and begins a new one. 


\section{Translation Into Programs}
At the outset of this work, it had been hoped that there existed some form of transformation from the language defined by creating a formal description of an unambiguous subset of the user gestures to a potential language of robot behaviors. 
While the language of robot behaviors is itself not terribly well-defined, approaches such as the flavors of AutoMoDe and Supervisory Control Theory hint that the output language would likely be able to be represented as a DFA or PA, and so the resulting programs could then be amenable to analysis using a model checker such as PRISM \citep{KNP11}.

However, the user gesture language as defined by this work is actually quite vague, when it comes to allowing a robot to perform the expected actions. 
Users were told to assume that the system was capable of understanding their orders, so they merely had to indicate what they wanted the system to do, and it would then do it. 
Alan Perlis has been quoted as saying ``When someone says `I want a programming language in which I need only say what I wish done,' give him a lollipop.'' \citep{PerlisYaleLolz}.  
Such systems are substantially more difficult than the recipients of lollipops expect, because they rely on a large amount of \emph{a priori} information shared between the person issuing the command, and the system executing it. 

For example, in the box-moving task, the users would frequently move the robots to surround the box, and then move the robots to the goal area. 
However, one might expect that robot control programs would attempt to avoid obstacles, and so would just go around a box. 
Without the knowledge that boxes are acceptable to push against, no motion would occur, and the robots would in fact actively resist being steered to push the box. 
Even this knowledge shows the limitations of the gesture as a way of conveying a program to a robot.
The user data set does not have clear gestures for conveying that box-pushing is desirable, how to recognize the presence of a box, how to tell one box from potential other boxes, or how to convey that any particular object can be pushed, rather than just boxes. 
Instead, users seemed to assume that the robots understood, as the user did, that the box was a thing that could be moved, and so did not have to be told.

Because the user gestures did not convey all of the information required to perform tasks, there is not a transformation that could operate purely on the user input to produce a program as output. 
Instead, the output program also has to include elements of planning and other knowledge. 
Because the system was intended to operate in a potentially unknown environment, the bug, dispersion, and occlusion-based transportation algorithms discussed in the previous section were used. 
If the environment were known, or communication were assumed to be reliable, other algorithms could be used. 
Indeed, the translation layer could be modified to switch algorithms based on the parameters of the swarm it is creating programs for and their environment. 

The development of the translation layer was performed in a manner similar to a compiler, which permitted the planning and other algorithms to be built into output programs by the translation layer. 
The input language was the user gestures, including which robots were selected and which user-specified paths were created. 
These inputs were used to parameterize the chosen algorithms, and the robots selected were used to determine the distribution of the resulting programs.
As a result, this work does not end up breaking away from iterative hand-coding, it just moves it from being done as a way of controlling the swarm, to being done as part of the creation of the control interface. 
For the motivating example from the introduction, urban search and rescue, this is acceptable, as it does not require the end user to program the swarm. 
It is also somewhat risky, as the resulting system may not have the flexibility that end users require. 

\section{Implementation Details}

User gestures arriving at the translation layer are stored in a stack until an end-of-command gesture arrives. 
The gesture sequence is then translated into a program that is parsed by the Lark parser library. 
Lark is an Earley parser, and so can parse all context-free grammars, although the current gesture language is not sufficiently complex to actually need this level of power. 
The resulting parse tree is then walked to generate GCPR programs that implement the algorithms described in Chapter \ref{chapter:Implementation_of_Swarm_Actions}.

Basic movement to points is implemented using the variant of TangentBug. \todo{address real-world implementation of tangent bug, also GCPR composition}
Path following and formation combines the basic movement to points with a sequence of GCPR instructions that implement a program counter, and set the goal based on the program counter. 
As each point is either reached or determined to be unreachable, the goal is advanced to the next point.
For motion along paths, the goals are set to points along the path, ending at the final goal. 
Formation allows the robot to stop at reachable points on the formation, but not unreachable points.  

Patrol also uses modified TangentBug, but instead of terminating when the program counter, and so the goal, reach the final position, the program counter and goal are reset to the start point of the patrol.  
As a consequence, the resulting program intentionally contains an infinite loop, but it can be interrupted by assigning a new program to the swarm. 

Dispersion is implemented using a minimalistic range sensor. 
Each robot can detect if there are other robots within a fixed range. 
If there are more than two robots in the range, the robot moves forward, avoiding obstacles. 
If there are exactly two robots in range, the robot stops. If there are less than two robots in range, the robot executes a U-turn and drives in a straight line, avoiding obstacles, until one of the other situations occurs. 
This algorithm is a GCPR implementation of the $\alpha$-algorithm of Winfield \emph{et al.}, and so shares its strengths and weaknesses  \citep{winfield2008modelling}.
Notably, the algorithm is not certain to prevent the separation of the swarm into subswarms that are not connected. 
More sophisticated programs, possibly using more communication, can prevent these issues, but since the requirement of the behavior is that the robots disperse, rather than that they maintain a particular level of network connectivity, there is no need for this particular implementation to enforce connectivity. 
Indeed, simply moving the robots to random locations uniformly selected would ``disperse'' them. 
However, selecting points would require foreknowledge of the area to disperse into. 
The $\alpha$-algorithm was chosen instead because it can operate in a previously unknown area, and because the resulting distribution looks even, visually. 
A randomly selected set of points from a uniform distribution may place two robots right next to each other, which, while ``uniform'' in a statistical sense, would appear uneven to the user, and not satisfy their intuitive understanding of dispersion. 
If dispersion in swarm robots continues to be a problem of interest, it is likely worth investigating the tradeoffs between speed of convergence, quality of dispersion, and user satisfaction with various methods. 

Manipulation was implemented as a simple version of occlusion-directed transport. 
If the robot is not near the target object, it attempts to move to the target object while avoiding obstacles, using the modified TangentBug algorithm. 
When the TangentBug algorithm detects that the goal is unreachable, because it is inside of the mobile object, the program switches to occlusion-based manipulation. 
The robot wall-follows around the object until a line from the robot to the goal intersects the object, and then pushes in that direction. 
While in this mode, the robot continually updates the direction of the goal, and switches between getting in position and pushing the object, as needed. 
As with the original occlusion-based manipulation, this algorithm is ignorant of obstacles on the opposite side of the object from the robot, and so can get stuck. 
However, as the system can accept a path for the object to be pushed on, the user can attempt to specify a clear path for the robots to move the object along. 

\section{Interpretation of Programs}

%\chapter{Compiler Verification}
%
%The design of a compiler which verifies that its output is correct What does it mean for a program to be correct? Free from things like assignment of variables to the wrong type, buffer overflows, off-by-one errors, etc. Does not mean that the program does what the programmer intended, just that it does whatever it does without errors. 
%Correctness is judged with respect to a specification. 
%Partial correctness - if an answer is returned, it is correct. 
%Total correctness - requires that program is partially correct and terminates
%No general solution to halting problem, needs specific proof per-program
%Compiler correctness shows that a compiler behaves according to its specification
%I don't currently have a language specification for the gesture language 
%
%One method of testing compilers is the operation of a fuzzer \citep{miller1990empirical}. A fuzzer, in the general case, generates input for programs that, while technically legitimate input, causes unexpected behavior such as program crashes. A fuzzer for compilers uses a description of the source language to generate programs that are considered valid under the description, and then attempts to compile them using the compiler. It can also attempt to generate programs that are deliberately incorrect, to test the ability of the compiler to accurately report errors, and to ensure that the compiler accepts all correct programs and no incorrect programs, as in \citep{bazzichi1982automatic}. 
%Because the internal structure of the compiler is not known, a generator cannot be certain that it causes all code paths in the compiler to be executed, but it can be certain that the generated test programs use all of the syntactical elements of the language, since all of the elements are described in the language specification. Interestingly, Bazzichi and Spadafora's work predates the use of the term ``fuzzer'' by 8 years, but clearly describes the same process.
%
%Translation validation \citep{pnueli1998translation}
%verifies compiled code against input code rather than attempting to verify that a compiler correctly translates all input programs.
%Verifies that the compiler correctly transforms a specific input program. 
%Needs:
% - A common semantic framework for the representation of the source code and the generated target code (probablistic finite automaton?)
% - A formal description of "correct implementation"
% - A proof method for verifying that one instance of the semantic framework (the output) correctly implements another one (the input)
%How is the IR not the common semantic framework?
%
%\section{Limitations}
%
%input semantics don't include way for user to set robot heading, might be useful for sensor overwatch




