% !TeX root = ams_thesis.tex
\chapter{Contributions} \label{chapter:Contributions}

\subsection{Multitouch Gesture set for Swarm Control} \label{section:Multitouch_Gesture_set_for_Swarm_Control}
This work will attempt to determine if there is an multitouch gesture set for a single user to command a large swarm of robots.
For the experiment, there are 5 cases: 1 robot, 10 robots, 100 robots, 1000 robots, and cloud (unknown robot count). 
In all the cases except the cloud case, the robots are represented as individual robots on the screen of the interaction device. 
In the cloud case, the robots are presented as a cloud covering the area that the robots are present in, but without precise representation of each individual robot. 

There are 18 tasks. 
Each user is assigned to one case, and then performs all the tasks using that case. 
Having each user work on one case is intended to prevent the user from being influenced by their memory of what they did in a different case, and so creating a consistency that is not a product of the requirements of the task and case. 
Some of the tasks do not make sense with the cloud case, such as interactions with a single robot, so users who are assigned to the cloud case will not view the tasks that are impossible in that case.
Pilot runs of the study indicate that it can be completed in one hour. 

The data set will be analyzed and compared to existing work on multitouch gestures for command and control. 
The analysis will also attempt to determine if the gesture set shows a transition point between many and few robots. 
The influence of the presentation of the interface on the gesture set will be examined. 
The gesture set will also be analyzed to determine if the size of the swarm has any effect on the gestures used, or on neglect of individual robots by the user. 

\subsection{Compilation of User Gestures into Robot Programs} \label{section:Compilation_of_User_Gestures_into_Robot_Programs}
The automatic conversion from a user-specified task into a set of command programs to be distributed to the swarm robots is still an open question.
One recent approach uses a human-in-the-loop multitouch interface to allow a human to guide a swarm by drawing a bounding prism that the swarm attempts to remain within \citep{ayanian2014controlling}. 
As the bounding prism moves, the swarm moves with it, with the individual robots performing obstacle avoidance. 
However, this work assumes that the individual swarm units can localize themselves, and that there is constant availability of communications between all swarm members and the central controller. 
For a number of reasons, these assumptions frequently fail to hold, and so a more robust system can be designed by assuming that localization and communication are difficult. 
This work will attempt to create an automated process by which user-specified behaviors of the swarm as a whole can be converted into programs that run on individual robots. 
The behavior of the individual robots under this control should converge to the user-specified behavior without further communication from the central server.

Previous work in gesture control for small groups of robots was able to recognize a grammar of user inputs using a finite state machine \citep{micire2010multi}.
The intent of this work is to implement basic robot behaviors as statements in an implementation of guarded control programming with rates (GCPR), which can then be composed into more complex behaviors based on the recognized user gestures \citep{napp2011compositional}.  


