% !TeX root = ams_thesis.tex
\chapter{Contributions} \label{chapter:Contributions}

\section{Swarm Hardware and Software Platform}

A hardware platform for control of small, inexpensive swarm robots was developed. 
The fact that swarm hardware across the literature has roughly the same general layout for the hardware can be viewed as an example of similar needs leading to similar solutions.
Gianpiero \emph{et al} describe a reference model for the robots in their work, which is inspired by the E-puck \citep{francesca2014automode}.
The reference model could probably generalize well to other hardware, as the E-puck on which it is based also has a ring of IR sensors and differential drive. 
The GRITSbots, Amir, and Colias all have a ring of IR sensors, as do many other designs. 
Most swarm robots use differential drive for steering, through either a sealed gearbox or direct drive.
A generalized reference model would also be useful for developing SCT generators for robot control, as the set of free behavior models is specific to the robot, and so could be reused for new tasks using the reference system. 

TinyRobo aimed to drive the cost of this style of robot down further, by virtualizing the sensors. 
The TinyRobo ROS modules include the ability to have configurable virtual laser scanners, range and bearing sensors for inter-robot sensing, and virtual networks, all of which can be easily customized to allow experimentation with sensor noise or failure. 

The use of children's toys as a mobility platform does not result in substantial savings, or even in easier assembly of the platform. 
Toys also have reliability problems that offset their possible utility as mobility platforms. 
However, if some other drivetrain is used, the power and control module is still cheap, small, and easy to use. During the course of this work, 3D printers have dropped substantially in price, so the difficulty of producing custom small mechanical assemblies has been significantly reduced since the time of the development of e.g. the Jasmine micro-robots. 
As a consequence, the use of a 3D printed robot chassis and the TinyRobo control modules can produce a very inexpensive swarm platform. 

%Assuming the toys are replaced with either stepper motors or small DC motors with built-in gearboxes, the complete hardware platform is similar to Colias or the GRITSbots.
%A review of online suppliers at the time of this writing indicates that small stepper motors designed for use in cameras area available for well under one dollar per unit, so this change would actually result in substantial cost savings. 
%However, it would require alteration of the drive electronics to control a stepper motor. 
%Small DC motors with sealed gearboxes are available for \$3-5 in single quantities as well, so combining them with a 3D printed enclosure would likely yield a more reliable base, at a similar price to the toys used in this work. 
Resolving the problems with that this work ran into with AprilTags would result in an approach closer to mROBerTO.
The motor drive electronics would remain the same, but either fixed color tags or LEDs as identity indicators would be used to track the robots. 
It has been suggested that a ring of addressable LEDs could be used to convey information from the swarm to the control system, by changing the color of LEDs on the ring or animating them in patterns. 
Since such a display could also be meaningful to the user, this may be an interesting direction for future research in HSI for co-located swarms.  

Because of the modular nature of the system, the ROS stack can be interfaced with simulations as well as with real robots. 
The main feature making this possible is the fact that the communication with the swarm hardware operates using standard ROS modules as much as possible, and only performs hardware-specific operations where they cannot be avoided. 
This flexibility enabled an easy transition to testing in simulation when it became apparent that the toy bases were not going to become a useful platform. 

\section{Multitouch Gesture set for Swarm Control} \label{section:Multitouch_Gesture_set_for_Swarm_Control}

Gestures were collected from 50 users to define a gesture set for multitouch swarm control. 
Analysis of the data showed that there were variations in the uses of certain gestures as the size of the swarm increased. 
In particular, the use of selection gestures increased in the 10 and 100 robot cases, but dropped off for the 1000 robot swarm. 
Tap gestures were more likely to be used for selection in the unknown, 1, and 10 robot cases than the other cases, while group selections were used mainly in the 10 and 100 robot cases, and less in the 1000 robot case. 
The 10 robot case seems to be the transition point where use of group or single tap selections are equally used. 

Showing the area occupied by the swarm as a cloud had very similar use of selections to the one robot case. 
Box selection was never used, lasso was used rarely, and tap selections were the most common selection gesture by far. 
These changes to the user's choices of gesture support the hypothesis that the gesture selection does change with the user's perception of the swarm, both the visible number of robots and whether the swarm is rendered as individual robots or a coverage area. 

There is evidence from the user survey and gesture selection that video games and other prior experiences with multi-touch interaction devices have an influence on the gestures used. 
The interface design used in the experiment was initially somewhat like the interface of a realtime strategy game, and as a result, seems to have cued users who had played realtime strategy games (RTSs) to use styles of interaction common to RTSs.
Designers of future interfaces can interpret this as both a strategy and a warning. 
As a strategy, interfaces can be designed specifically to include design features from games, and so cue the users that the interface supports the interactions that are common in the genre of game that the interface emulates. 
However, it can also be a warning that if an interface resembles a game, the users will expect those interaction styles, and may entirely avoid other interaction styles that do not fit with their expectations. 
The total absence of pinch gestures on the part of RTS gaming users is such a missing interaction. 
At a more abstract level, designers would be wise to avoid leaning too heavily on game-based cues in user interface design, unless they are certain that their users are gamers, or the cue will be missed. 

Despite being told that the device is multitouch, most users made very few two-handed gestures, although half of the users made at least one two-handed gesture.  
Voice commands were more common in the data set collected for this experiment than in previous experiments that allowed users free reign in choice of their command set. 
It is speculated that this is due to the rise in functionality and prevalence of ``voice assistant'' technologies in smartphones and home appliances such as Google Home. 
This transition would be similar to the transition observed in previous work, where people who had smartphones used pinch gestures far more than people who did not have experience with smartphones or similar multitouch devices. 

It had been surmised that one possible sign that the user was treating the robots as a group would be that some parts of the group would be neglected. 
This generally did not occur. 
Instead, the users used selection gestures that included all robots, and when asked about inclusion in gestures, erred on the side of including more robots. 

There were also relatively few gestures treating the robots as a deformable mass that could be pushed around, like a pile of sand or other small objects. 
Physical affordances like this were speculated to be more likely, given the direct interaction style of the multitouch screen. 
Their absence could be viewed as highlighting the users' understanding of the screen as an image, and so not something that supports physically-afforded interactions. 

In future, it would be interesting to repeat this work with a condition that does not display the robots in the user interface at all. 
We expect that for conditions such as the ``move the crate'' tasks, the user would simply indicate the crate should move to area A, without concern for which robots perform the moving. 
However, such an interface would not afford indicating particular robots or groups, so tasks such as dividing the robots around an obstacle may become impossible to perform. 

\subsection{Compilation of User Gestures into Robot Programs} \label{section:Compilation_of_User_Gestures_into_Robot_Programs}

This project shows a basic conversion from user gestures into a set of command programs to be distributed to the swarm robots.
These command programs are intended to balance desirable formal properties of the programs, such as convergence within bounded time, use of local-only sensing, and completeness, with reflecting the user's intentions in the observable behavior of the swarm. 

In attempting to derive complete program frameworks for the tasks specified in the user studies, it was determined that complete controllers for some simple tasks may not exist. 
For example, while it is possible to have a complete controller for motion to a point, it is impossible to have a complete dispersion controller that relies on only local sensing. 
In cases where completeness could not be determined, or was determined to be impossible, the controllers used are developed to use local-only sensing and if they fail, to do so in a manner that is intelligible to the user. 

The translation between user commands and robot programs in this work was developed using a method similar to compiler development, where an input language was defined from the user gesture commands, and an interpreter was developed to take strings in the language of gestures, and output control programs in as statements in an implementation of guarded control programming with rates (GCPR) \citep{napp2011compositional}.  
Previous work in gesture control for small groups of robots was also able to recognize a grammar of user inputs using a finite state machine \citep{micire2010multi}.
It had been initially hoped that there would be a universal abstraction or transformation that could represent the conversion of all gestures into all robot commands representing those gestures, rather than having different sets of behaviors, with different properties, which could be constructed from the user gestures after recognizing those gestures. 
However, user gestures do not provide a sufficiently explicit means of designing a program in realtime to allow for the full automation of the generation of control programs without assuming an amount of \emph{a priori} knowledge that is unrealistic in practice. 
For example, while there is a gesture for disperse, the gesture itself does not contain information about how the dispersion should be performed. 
Similarly, while the gestures used to indicate that an object should be moved to a location contain information about what the movement should be, they do not contain information about how the movement should be accomplished. 
Because these elements are not present in the user input, any universal transformation that has them in its output would have to essentially make them up. 
As a consequence, the translation was implemented in the way that programming languages are usually implemented, with a developer supplying the method that the robots would use to fulfill the user's commands. 

There are a number of threads in current swarm robotic research that may eventually permit the engineering of swarm behavior from high level descriptions, many of which are discussed in section \ref{section:Swarm_Software_Development_Methods}. 
One that is not discussed in great depth there is the field of attempts to find mappings between relatively low-dimensioned swarm macrostates and potentially extremely high-dimensional state spaces representing each robot individually. 
While some work has been done in controlling swarms with controllers operating on macro-level behavior, such as attractors, there is not yet an engineering discipline that would guide the creation of arbitrary swarm behaviors under real-world complexity \citep{brown2014human}. 
As this area of swarm robotics develops, user interfaces will have to develop with it, in order to allow humans to operate these systems in a fluent manner. 