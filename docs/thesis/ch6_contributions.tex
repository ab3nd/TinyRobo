% !TeX root = ams_thesis.tex
\chapter{Contributions} \label{chapter:Contributions}

\section{Swarm Hardware and Software Platform}

A hardware platform for control of small, inexpensive swarm robots was developed. 
The fact that swarm hardware across the literature has roughly the same general layout for the hardware can be viewed as an example of convergent evolution. 
The GRITSbots, Amir, and Colias all have a ring of IR sensors, as do many other designs. 
Most swarm robots use differential drive for steering, through either a sealed gearbox or direct drive.
Gianpiero \emph{et al} describe a reference model for the robots in their work, which is inspired by the E-puck \cite{francesca2014automode}.
The reference model could probably generalize well to other hardware, as the E-puck on which it is based also has a ring of IR sensors and differential drive. 
A generalized reference model would also be useful for developing SCT generators for robot control, as the set of free behavior models is specific to the robot, and so could be reused for new tasks using the reference system. 

TinyRobo aimed to drive the cost of this style of robot down further, by virtualizing the sensors. 
The TinyRobo ROS modules include the ability to have configurable virtual laser scanners, range and bearing sensors for inter-robot sensing, and virtual networks, all of which can be easily customized to allow experimentation with sensor noise or failure. 

The use of children's toys as a mobility platform does not result in substantial savings, or even in easier assembly of the platform. 
Toys also have reliability problems that offset their possible utility as mobility platforms. 
However, if some other drivetrain is used, the power and control module is still cheap, small, and easy to use. During the course of this work, 3D printers have dropped substantially in price, so the difficulty of producing custom small mechanical assemblies has been significantly reduced since the time of the development of e.g. the Jasmine micro-robots. 
As a consequence, the use of a 3D printed robot chassis and the TinyRobo control modules can produce a very inexpensive swarm platform. 

%Assuming the toys are replaced with either stepper motors or small DC motors with built-in gearboxes, the complete hardware platform is similar to Colias or the GRITSbots.
%A review of online suppliers at the time of this writing indicates that small stepper motors designed for use in cameras area available for well under one dollar per unit, so this change would actually result in substantial cost savings. 
%However, it would require alteration of the drive electronics to control a stepper motor. 
%Small DC motors with sealed gearboxes are available for \$3-5 in single quantities as well, so combining them with a 3D printed enclosure would likely yield a more reliable base, at a similar price to the toys used in this work. 
Resolving the problems with that this work ran into with AprilTags would result in an approach closer to mROBerTO.
The motor drive electronics would remain the same, but either fixed color tags or LEDs as identity indicators would be used to track the robots. 
It has been suggested that a ring of addressable LEDs could be used to convey information from the swarm to the control system, by changing the color of LEDs on the ring or animating them in patterns. 
Since such a display could also be meaningful to the user, this may be an interesting direction for future research in HSI for co-located swarms.  

Because of the modular nature of the system, the ROS stack can be interfaced with simulations as well as with real robots. 
The main feature making this possible is the fact that the communication with the swarm hardware operates using standard ROS modules as much as possible, and only performs hardware-specific operations where they cannot be avoided. 
This flexibility enabled an easy transition to testing in simulation when it became apparent that the toy bases were not going to become a useful platform. 

\section{Multitouch Gesture set for Swarm Control} \label{section:Multitouch_Gesture_set_for_Swarm_Control}

Gestures were collected from 50 users to define a gesture set for multitouch swarm control. 

Analysis of the data showed that there were variations in the uses of certain gestures as the size of the swarm increased. 
In particular, the use of selection gestures increased in the 10 and 100 robot cases, but dropped off for the 1000 robot swarm. 
Tap gestures were more likely to be used for selection in the unknown, 1, and 10 robot cases than the other cases, while group selections were used mainly in the 10 and 100 robot cases, and less in the 1000 robot case. 
The 10 robot case seems to be the transition point where use of group or single tap selections are equally used. 

Showing the area occupied by the swarm as a cloud had very similar use of selections to the one robot case. 
Box selection was never used, lasso was used rarely, and tap selections were the most common selection gesture by far. 
These changes to the user's choices of gesture support the hypothesis that the gesture selection does change with the user's perception of the swarm, both the visible number of robots and whether the swarm is rendered as individual robots or a coverage area. 

There is evidence from the user survey and gesture selection that video games and other prior experiences with multi-touch interaction devices have an influence on the gestures used. 
Despite being told that the device is multitouch, most users made very few two-handed gestures, although half of the users made at least one gesture.  

Voice commands were more common in the data set collected for this experiment than in previous experiments that allowed users free reign in choice of their command set. 
It is speculated that this is due to the rise in functionality and prevalence of ``voice assistant'' technologies in smartphones and home appliances such as Google Home. 
This transition would be similar to the transition observed in previous work, where people who had smartphones used pinch gestures far more than people who did not have experience with smartphones or similar multitouch devices. 

In future, it would be interesting to repeat this work with a condition that does not display the robots in the user interface at all. 
We expect that for conditions such as the ``move the crate'' tasks, the user would simply indicate the crate should move to area A, without concern for which robots perform the moving. 
However, such an interface would not afford indicating particular robots or groups, so tasks such as dividing the robots around an obstacle may become impossible to perform.

During this experiment, there were some cases where the users expectations of what was possible in the interface indicated a sort of ``metaphor failure" in the user interface. 
Natural User Interfaces, of which multitouch screens are an example, are supposed to be able to leverage users' understanding of the physical world, and how objects behave in it, to build affordances for objects on the screen. 
For example, a volume knob can be displayed as an actual knob, and the screen can react appropriately to attempts to rotate the knob. 
However, people know that the objects on the screen are not e.g. knobs, switches, and so on, but pictures of those things, drawn by the computer. 
As a result, the affordances are mixed. 
The knob may afford turning, but it also affords dragging around the screen or deletion, which a knob on a real radio does not afford. 
In this study, users were tasked with stopping the robots, which had begun to move around a wall to a target area. 
One user dragged the wall in front of the robots, and another user asked if they could move the wall. 
The wall was intended to represent an actual wall, which does not afford moving in the physical world, but it was represented in the experiment as a thick black line. 
It may be that the users would have not attempted to move the wall if it was more clearly represented as e.g. a stone or brick wall, and so had connotations of excessive weight. 
On the other hand, the users may have regarded it as what it actually was, an image of a wall on a computer screen, and decided that since images can be moved around the screen, the wall image can too. 
Attempting to experimentally determine if the representation affects how the user interacts with the wall may require a large number of users, as out of 50 participants, only 2 even mentioned the idea of moving the wall. 

In this experiment, some users were initially confused by the interface not responding. 
It may be that running this experiment on a computer, rather than a paper prototype, contributed to the user expectation that the system could react. 
Most users' experience with touch screens is that when they touch them, something visible happens nearly immediately. 
A system that does not visibly react, as in this experiment, is usually assumed to be broken, or waiting for further input, but no one expects printed documents to react to touch.
For future work, it may be desirable to structure attempts to elicit user gestures as in Wobbrock \emph{et al.}. 
The experiment described in this section showed the initial situation, and asked the users how they would make a specific change. 
Wobbrock \emph{et al.} showed the change occurring, and then asked the users what command they would issue to cause that result. 
Showing the response before asking for the gesture removes the expectation that the system will react. 

Unfortunately, showing the response of the system may also act as a cue to the user that suggests a specific solution for gesture selection. 
For example, if the system shows a square formation being formed by multiple robots moving directly to the closest point on the square to their starting location, the paths shown are multiple direct motions. 
If instead, the system shows the robots forming a chain that snakes around the perimeter of the square before coming to a stop, the path shown is forming the snake, and then traversing the perimeter. 
The motion in the first case may suggest that the user make individual gestures to position each robot, while the gesture suggested in the second case might be more like dragging a lasso around the group and then a line from the group around the perimeter of the box. 
Even simple cases like moving to a target area, if the robots are shown moving along a path, it might discourage the user from using waypoints instead of dragging a path.

\subsection{Compilation of User Gestures into Robot Programs} \label{section:Compilation_of_User_Gestures_into_Robot_Programs}

This project shows a basic conversion from user gestures into a set of command programs to be distributed to the swarm robots.
These command programs are intended to balance desirable formal properties of the programs, such as convergence within bounded time, use of local-only sensing, and completeness, with reflecting the user's intentions in the observable behavior of the swarm. 

In attempting to derive complete program frameworks for the tasks specified in the user studies, it was determined that complete controllers for some simple tasks may not exist. 
For example, while it is possible to have a complete controller for motion to a point, it is impossible to have a complete dispersion controller that relies on only local sensing. 
In cases where completeness could not be determined, or was determined to be impossible, the controllers used are developed to use local-only sensing and if they fail, to do so in a manner that is intelligible to the user. 

The translation between user commands and robot programs in this work was developed using a method similar to compiler development, where an input language was defined from the user gesture commands, and an interpreter was developed to take strings in the language of gestures, and output control programs in as statements in an implementation of guarded control programming with rates (GCPR) \citep{napp2011compositional}.  
Previous work in gesture control for small groups of robots was also able to recognize a grammar of user inputs using a finite state machine \citep{micire2010multi}.
It had been initially hoped that there would be a universal abstraction or transformation that could represent the conversion of all gestures into all robot commands representing those gestures, rather than having different sets of behaviors, with different properties, which could be constructed from the user gestures after recognizing those gestures. 
Supervisory Control Theory may provide an approach to reducing the amount of hand-developed code in the system, and so human error. 
However, user gestures do not provide a sufficiently explicit means of designing a program in realtime to allow for the full automation of the generation of control programs without assuming an amount of \emph{a priori} knowledge that is unrealistic in practice. 


