% !TeX root = ams_thesis.tex
\chapter{UI Design and Implementation}

In order to convert the user gestures collected in the gesture collection experiment into a user interface, a subset of the gestures was selected to support the majority of the user-defined gestures. 

In some cases, the user defined gestures were ambiguous, either across users, or with a user across tasks. 
Where design decisions were made to work around these ambiguities, they are described. 

\section{Selection of Gestures for Control of Swarms}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{../thin_grey_text.png}
	\caption{Motion, selection, and UI gestures.}
	\label{fig:select_ui_move_breakdown}
\end{figure}

For position commands, drag, tap, and ``other'' would cover 94.8\% of the gestures. 
Unfortunately, the ``other'' commands are not a single gesture, but include a diverse collection of gestures, such as pushing with the side of the finger or making a picking-up, carrying, and setting-down motion over the screen.
As a consequence, attempting to implement all the gestures in the ``other'' category would add significant complexity to the gesture recognition in order to support gestures which were rarely used. 
Many of the ``other'' gestures were also not recognizable by a multitouch surface, such as the user dividing the robots by parting their hands as if opening a bag in the air above the surface. 
Since it does not contact the multitouch surface, such a gesture cannot be recognized without additional hardware. 
If all forms of tap are considered the same when used as position commands, taps make up 14.2\% of the position commands, and together with drag, cover 89.2\% of the position commands. 

Lasso was used as a position command by one user, who used it to command the robots to disperse in the ``disperse'' task. 
The user noted that the direction of the lasso disambiguated it from lasso as selection, so lassoing clockwise would select robots and lassoing counterclockwise would disperse them. 

Achieving 90\% recognition of the most common UI widgets would include buttons for special functions, handwriting recognition, voice commands, and other menus on the screen (totaling 90.8\%). 
However, a successful menu-based UI would not be composed simply by taking the sum of all of the user interface designs suggested by users, as there would be significant redundancy in the UI commands, and in ways of bringing up the UI for interaction. 
Instead, the tasks in which the users invoked the UI most often were examined, and the requested UI functions were considered for inclusion.

Both voice commands and handwritten commands together account for 47.1\% of the user interface elements used.
While the broad categories make up nearly half of the UI elements, the individual uses of the gestures did not display a significant unity within the broad categories.
One user used symbols drawn over the swarm as commands, so a circle drawn within the swarm area meant "patrol", and would be followed by an indication of the area to be patrolled. 
Another user wrote commands, such as writing out ``0.5" to indicate that robots should divide in half, ``SQ'' to indicate that they should form a square, or ``C $\rightarrow$ A" to indicate that the crate (C) should be moved to area A. 
The majority of the drawn commands were attributable to a single user, but two users drew an `X' over the defective robot in the ``mark defective robot'' and ``remove defective robot'' tasks and two users used an `X' and an `S' in the ``stop the robots'' task.

The selection of a set of handwritten commands that would be usable for a majority of users would be a task at least as difficult as finding a user-defined set of gestures.
In addition to the set of English letters, the interface would have to consider symbols such as arrows, and apparently decimal numbers as well. 
The experiment described in this thesis did not collect sufficient examples of handwritten commands to extrapolate a useful control scheme from them.
Because of this lack of information, and the fact that most handwritten commands were proposed by a single user, the resulting interface does not contain support for handwriting recognition and symbol interpretation.  

Interestingly, voice commands were similar to both handwriting, in terms of user choice, and gestures, in terms of syntax. 
One user used voice commands for all tasks, but a few users also used voice commands for a few tasks, but not all of them. 
This distribution is like the handwritten commands, in that one user used handwritten commands frequently, but a few other users used a few of them for some tasks.
The accidental biasing of users towards use of voice for formation tasks was discussed earlier. 
In addition, two users stated that they would like to have a vocal stop command for the ``stop the robots'' task, because it could be issued quickly and without having to accurately touch the moving robots. 
These users did not issue other voice commands, as the other tasks were not perceived to be as urgent as the ``stop the robots'' task. 
The syntactic similarity between the voice commands and the gesture commands is because the gesture commands frequently take the subject-verb-object ordering of spoken sentences. 
For example, selecting the robot group indicates a subject, and drawing the path indicates the verb (``go this way"). 
Objects are optional or implied, as going to a location does not have a clear object that the robots are instructed to act upon. 
In some cases, the subject is implied as well.
For example, some users would make gestures intended to move the robot group as a whole by simply dragging the path, without selecting the subject first. 
In such a case, the implied subject was all of the robots. 
This sort of implication is more complex than simply assuming that if no robot is selected, all of the robots are the subject. 
Some users divided the robots into two groups by drawing a dividing line, and then dragging two paths, one to one side of the screen and the other to the other side of the screen. 
In this case, the implied subject was the half of the robots on the same side of the dividing line as the drag, but no selection gesture preceded the positioning drag to indicate this. 

%The idea of having a hidden menu that was dragged in from an edge of the screen or came up after some form of invocation, such as tap-and-hold, was rejected because such menus do not admit exploration. \todo{cite something on explorability/discovery of menus}

Reverse pinch was used as an interface command to zoom in or out of the view of the robots. This was done in the ``mark defective robot'' and ``remove defective robot'' tasks. 
The user who made this gesture was in the 1000 robot case, and stated that they wanted to zoom in because the defective robot was a small target and close to other robots. 
There was no explicit zoom or change of viewpoint task, but users expected the functionality when they had to interact with a small target.

To accommodate 90\% of the user gestures for selection, recognition of tap, lasso, box select, doubletap, drag, hold, and ``other'', totaling 91.9\% of the selection gestures.
As discussed above, the ``other'' category is not practical to implement. 
If, instead, all forms of tap are considered the same for purposes of selection, they comprise 42.5\% of the selection gestures, and together with lasso and box select, cover 91.3\% of the selections. 

Drag as a selection gesture refers to the user placing their finger on one robot, and then dragging it from robot to robot, adding each touched robot to the selection. 
It is ambiguous with the position drag, where the user places their finger on a robot and then drags a path for the robot to follow, although the distinction could be made by having a path that intersects multiple robots become a selection, rather than a position command. 
Unfortunately, this attempt at disambiguation would itself become a source of problems if the user attempts to move the entire swarm by placing a finger in the middle of it and dragging to a new location, as it is likely they would intersect more than one robot as their finger leaves the swarm.

\section{Ambiguities in Gesture Commands}

As discussed briefly in the previous section, some combinations of gestures selected by the users were ambiguous. 
This is to be expected, as the users did not know all the tasks in advance, and so might use a gesture in one task that they then felt was better suited to a later task.
The users might also not regard all the tasks as having to use a consistent and unambiguous representation for each gesture, or not remember all the gestures they had previously used. 
However, for automated conversion into programs, it is important that a sequence of gestures have some way to be unambiguously recognized. 

In the line formation tasks, some users selected the robots and then drew a line to indicate the location of the line formation.
If the line formation started at the same location as the robots, this gesture sequence is the same as the selection and drawing a path sequence that was frequently used to indicate that the robots should move as a group along the path. 
If the line formation started elsewhere, it could be disambiguated, because motion along a path almost always started on or near the selected robots.  

Similarly, some users divided the robots into two groups by drawing a line separating the groups.
Typically, this line started outside the group and passed through it, so it could be disambiguated from instructing the robots to move along the line.
Unfortunately, it would be difficult to separate it from drawing a line for the robots to move to in order to create a formation.

To command the robots into a square formation, especially in the dispersed cases, many users simply drew the square formation over the robots.
If a lasso select is also available, some distinction must be made between the lasso select and the square formation when they are both drawn over the robots, as they are both closed forms drawn over the robots.
One possible method is to look for peaks in the distance from each point on the perimeter of the gesture to the centroid of the gesture. 
A square would have four peaks, while a circle would have no obvious peaks. 
However, this sort of recognition means that a square lasso select, or an arbitrarily-shaped lasso select that happens to have four peaks, would get misinterpreted as a formation gesture, while a command to get into a circular formation would be interpreted as a lasso selection. 
It would be preferable to be able to make arbitrary formations, and arbitrarily-shaped lasso selections, and have them be disambiguated by some other method. 
One method that users proposed was to have a ``draw formation" button, which would then cause the next form that the user drew to be treated as the perimeter of the formation. 
Some users also held one finger down at the start of the formation, while tracing the perimeter with the other finger. 
This approach as the advantage of not adding UI elements, but is not easily discoverable by a user examining the UI. 

In order to patrol area A or the screen border, users frequently dragged the robots as if issuing a basic motion command. 
Some users indicated that there would have to be an additional signal to the robots to keep moving on the patrol route, once it was assigned, but there was not an agreement as to what that command should be. 
Of the users who indicated a need to convey that the swarm keep patrolling, some repeated the patrol route drag multiple times, others ended the gesture with a tap or triple tap, still others used the direction of the patrol route drag (clockwise or counterclockwise) to separate following the path once from continuously following it. 

Some users reused gestures for different purposes, such as tapping a robot to select it, but also tapping a robot to remove it.
Because 30.2\% of the selection gestures were taps, it was decided that selection, which occurs often, would be done by tapping, while removal of a robot, which is rare, would be performed by a different action. 

The user drawing a line over a group of robots could be interpreted a number of ways: as a dividing line, as a box selection, as a path for a single robot to follow, as a path for a group of robots to follow, and as the location of a line formation. 
Because a number of other, more explicit commands are available for dividing robots into groups, such as moving one group away from the other, the division line interpretation was rejected. 
A line drawn over robots is treated as a box selection, unless it begins on a single robot, in which case it is interpreted as a path for that robot to follow. 
For the purposes of this work, beginning ``on'' a robot was defined as the beginning of the line being within 80 pixels of the location of the robot on the screen. 
80 pixels was selected because it is the approximate width of a human finger on the screen used in this work. 
Obviously, this distance will vary with the resolution of the screen, but the mechanism for obtaining the resolution and physical size of attached screens varies with operating systems, and is outside of the scope of this work, aside from noting that this parameter may vary.

The distinction between interpreting a line drawn over robots as a command for one robot to follow versus a command for multiple robots to follow may depend on the number of robots present.
As seen in section \ref{section:Selection_Gestures} the use of selection gestures was low in the 1000 robot case, but and much higher in the 10 robot case, implying that with a large robot group, it might make sense to default to using all robots if none are selected. 
The interface could even change depending on the number of units being commanded, with small groups requiring that all robots be selected before a command can be applied to them, and larger groups defaulting to issuing the command to all robots if some subset has not already been selected. 
However, care must be taken to avoid learning effects. 
If the user has previously been exposed to a condition where the system defaults to selecting all robots, they may develop an expectation that other conditions behave the same, and neglect to select robots, resulting in incomplete commands (or commands being applied to no robots). 
Interestingly, the opposite case, where the user makes a selection of all the robots, but it is not actually required because it is the default behavior, does not result in a surprise for the user. 
Because the failure mode encouraged by a system that requires explicit selection is less surprising than one that has implicit select-all, the system was developed to require selection to issue a command to multiple robots. 
As described above, issuing a command to a single robot may be done without selecting it first, by beginning the desired path on the robot. 

The gestures used by the users could be divided between selection and position gestures, within which there was a great deal of agreement, and more complex actions, such as patrol, formations, picking up or moving objects, removing robots, and selection by color. 

For selection, if the system accepts taps (of any form), lasso, and box select, 91.3\% of the user selection gestures are covered. For position, tapping (again, of any form) to set goals or way points and dragging paths will cover 89.2\% of user position gestures.

Because the more complex tasks had higher variety of gestures that could be used to convey the user intent, as well as some users performing the more complex tasks by repeated selection and position gestures, attempting to handle this variety of gestures leads to two problems with the discoverability of the interface. 

First, since each of the gestures was chosen by a smaller number of the users, some training must be performed to inform users who would not have chosen that gesture. 
This training could be done with a ``cheat-sheet'' that users could pull up for reference, or an animated introductory tutorial sequence. 
These options are both less desirable than having the system be, in as much as it can be, self-explaining. 
They separate the training of the user from the user's interaction with the interface, rather than having the interface make clear what can be done.

Second, if a large percentage of the gestures for the more complex tasks were implemented, in order to provide high coverage of the various options that users chose to perform those tasks, there would be a large set of gestures that have very specific meanings. 
Increasing the set of gestures that the system recognizes increases the chances for ambiguities, where the same gesture was selected for multiple roles by different users, and for errors, where the system misinterprets one gesture as another. 
This interferes with discoverability, as the system may react in different ways to what the user felt were the same actions, confounding the user's ability to learn a mapping between their actions and the results that the system produces. 

As a result, the complex tasks were assigned to buttons on the user interface. 
The use of buttons is more discoverable, as the button simply says on it what it does. 
Buttons do obscure some of the camera view that forms the main part of the user interface, but this problem is not as severe as it might appear. 
The area the robots operate in is either bounded or unbounded.
If it is bounded, it either fits on the screen or does not fit on the screen, and if it is unbounded, it does not fit on the screen.
If the area the robots are in does not fit on the screen, the user likely views it by panning or zooming in and out, and so can pan or zoom so that the area covered by the buttons is not an area which they have to interact with to perform a task. 
If the area the robots are displayed on and interacted with in does fit on the screen, it can be shrunk slightly, and the buttons can be placed so they don't cover the interaction area. 
In either case, the design of the interface can accommodate a few buttons without seriously harming the user interaction, although the unchecked proliferation of buttons could lead to design difficulties. 

\section{Termination of Commands}

Some of the potential gesture commands, such as selection of a group of robots followed by tapping waypoints for them to follow, do not have a clear termination, as the system cannot tell if the user is done tapping waypoints, or simply has not tapped the next one yet. 
One potential solution to this problem is to attempt to parse the user command once enough time has elapsed since the last gesture received. 
Using a timeout to commit the command was rejected for two reasons. 
First, the timeout can result in the system beginning to take an action which the user did not want. If the timeout is too short, it could result in a program being transmitted to the robots before the user is done specifying it. 
If it is too long, it may never be triggered, and so all of the user's gestures could be gathered into one, potentially uncompilable, program. 
Second, the timeout is could be invisible. From the user's perspective, this causes the system to appear to begin acting at random, which is undesirable. 
The timeout could be made visible, using a countdown timer, which may then make the user feel pressured to act. 

In order to avoid having the system act prematurely, the system treats a doubletap on open space as an "end of command" signifier.
If the hardware used in the system were capable of detecting it, the user placing their hands in their lap or away from the screen could also be used as a signifier that the user has completed entry of their command. 
Most users moved back from the screen slightly and removed their hands from the volume above it once they were finished issuing their command, and no users were observed to rest their hands on the screen while not issuing commands. 
However, this behavior may have been a consequence of the particular arrangement of the screen and user seating, so further work would be required to determine if it is a sufficiently robust indication of command completion. 

In addition to doubletaps, there are limited instances where it is safe for the system to assume that the user's command is complete. 
Because commands are generally of the form subject-verb-object, a new subject begins a new command. 
The exception to this rule is tap selections, which some users used to select multiple robots by sequentially tapping on them. 
Tap selections are accumulated until a gesture other than a tap selection is entered.
After a non-tap action is entered, a new selection gesture will begin a new command. 

