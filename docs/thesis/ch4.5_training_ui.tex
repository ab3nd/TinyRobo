% !TeX root = ams_thesis.tex
\chapter{UI Design for Trained Users}
\thispagestyle{fancy}

The work on which this study is built was motivated in part by the idea of using robots in search and rescue (SAR) operations, to explore damaged buildings, enter unsafe areas, and provide additional sensor data and coverage while reducing risk to first responders.
First responders are trained on the equipment they use, but such training may be infrequent, and as a result, complex control systems may not be remembered when the time comes to use the equipment \citep{micire2010multi}. 
Because a SAR situation may develop rapidly, there is little or no time available for retraining, and so an interface for use by first responders would prioritize easy learning of the interface and use with minimal training over richness of the possible commands.  

However, SAR is not the only domain in which swarm robots might be used, and so considering only the gestures which maximize the ability of untrained users ignores domains where it is possible to train the users extensively. 
In such a context, adding complexity to the gesture set could allow for additional expressiveness in the interface.

One such high-training environment is the space program. 
Many SAR responders are not primarily SAR professionals, but work primarily in other capacities. 
NASA astronauts' primary job is service as an astronaut, and they spend a significant amount of time in training for missions, including using mock-ups of the interfaces of systems that they will use during their missions. 
For example, operation of the remote manipulator on the International Space Station (ISS) begins with 30 hours of ``Generic Robotics Training" in a virtual environment, which is used to qualify them for future, more specialized, training on the specific arm used on the ISS \citep{liu2013predicting}. 
Some astronauts do not pass this test, and as a result, are not qualified to use the manipulator. 
NASA has developed the TLX (Task Load indeX), which is a widely accepted subjective metric for the physical and cognitive workload imposed on the human operator during a task \citep{hart2006nasa}.
As a consequence, NASA is a good example of an environment where highly-trained users could be expected to interact with a more complex interface, and where interfaces could be evaluated to determine if they reduce or increase the workload on the user, as in \citep{fong2006human}.


\section{On-line Training}

Because user experience of video games does have some effect on their use of the user interface described in this work, it is interesting to examine other aspects of video games, and how they might be applied to user interface design for swarm robotics. 

It has been argued that the use of game-like user interfaces is inappropriate for the design of applications, but better suited to training material \citep{thomas1994games}. The basis for using a game-like user interface for an application is that games are engaging, and as a result, using a similar interaction paradigm should yield an engaging application. 
However, the dissimilarities in user motivation make this approach less useful. 
Games are played for the sake of their own play, rather than to accomplish a task outside of the game. 
Challenge and complexity are controlled in the game to make the play more rewarding, but extending this to applications for external tasks means making the tasks more difficult than they have to be, which will be a difficult proposition for end users to accept. 
Additionally, work and play are strongly divided culturally, and making one like the other will be resisted by users. 
Thomas instead proposes that computer-based training materials are a better target for game-like user interfaces. 
Like games, training materials are intended for high involvement over short periods, and are interacted with for themselves, rather than as part of the completion of an external task. 

The majority of the work on video games and learning is centered around the outcomes of games developed to be educational, rather than on how games that are intended to be purely entertaining still educate their users in the use of the game itself.
Modern games frequently include a tutorial level at the beginning of the game in order to familiarize the user with the controls of the game. 
The tutorial level provides instruction for the user in performing game actions, and assesses whether those actions have been performed, in order to control progression to the next section of the tutorial. 
For example, a game that includes horseback riding as part of gameplay may provide the horse to the player during the tutorial level, and instruct the user in how to use the controls to mount the horse and direct its movement. 
However, games do not always provide tutorials, and the lack of tutorial content, or the low quality of tutorials when present has been cited as a potential usability issue in games \citep{pinelle2008heuristic}.

The tutorial level of video games is usually somewhat separate from the main content of the game, in that actions in the tutorial have limited or no consequences in future gameplay. 
This raises a potential issue for implementation of game-style tutorials in a robot control system. 
If the system allows untrained users to control real robots in a tutorial mode, the user may make an error that does have permanent consequences for the future use of the system, such as causing a collision between robots. 
If, instead, the system allows the user to operate a simulation, the design and development overhead of implementing the tutorial is quite large, for a component of the system that the user may use very rarely or never. 
This dilemma may restrict pre-use tutorials to training the user as to which gestures the system can recognize, and only permit a sketch of the possible robot reactions to those gestures. 

These forms of training center on the gameplay of a specific video game. 
However, as discussed in the section ``Video Game UI Design'', there are commonalities in user interface designs across games, particularly within a given genre of game, and similarities exist between the top-down views used in Real Time Strategy (RTS) games, and the top-down swarm view used in this work. 
Indeed, some users remarked that the interface was ``like Starcraft", which is an RTS game, and used Starcraft-like box selection gestures. 
This sort of similarity has been leveraged to control the IRobot Packbot using a controller from a Sony Playstation or Microsoft XBox, rather than the Packbot OCU. 
The Packbot OCU is large and has a large set of individual controls for each function of the robot. 
After significant use, Packbot operators learn complex tricks to maximize their ability to control the robot \citep{micire2011design}.
IRobot developed a method to control the packbot using a game controller instead, because soldiers already had practice using it from playing video games \citep{glaser2010impact}. 

Rather than including tutorials, the system could be developed with a basic level of functionality based on user elicited gestures, as in this work, and attempt to recognize certain relatively inefficient patterns of user activity. 
When such a pattern is recognized, the system would propose alternatives that extend beyond the easily-discovered gestures to offer greater finesse or complexity. 
For example, in the user experiment, some participants performed the Move to Area A task in the 10 robot case by selecting each robot in turn and dragging it to area A, while other participants made a selection gesture and then a single motion gesture. 
If the system detects a repeated pattern of single robot interactions, it could display a hint to the user that selection of multiple robots can be done, and indicate how to use it. 
In order to minimize annoyance, the user should be able to dismiss the notification, or prevent future notifications if they feel that they are sufficiently proficient in the use of the interface. 

The data set collected in the user gesture experiment provides an interesting sample of possible advanced gestures. 
Rather than being common across a large percentage of the users, these were gestures that a small number of users selected, in an attempt to provide additional expressive power. 
The rest of this chapter discusses the ``long tail'' of user gestures, and their possible use as ``superuser'' gestures after an appropriate training period. 

\section{``Other'' Gestures}

The ``other'' gestures are those that did not match any of the other codes as described in Appendix \ref{chapter:coding_defs}. 
The largest category of other gestures is gestures that were some form of ``sweeping'' the robots around, but because of the diversity of these gestures, there was little agreement from user to user. 
Some users made gestures over the screen, as if sinking their fingers into sand or a pile of small objects and dividing the pile, or cupping their hands to bring the pile back together. 
Others used the edge of a finger (typically the index finger) or the heel of the flat hand to push robots, as if they were sweeping them along a countertop. 
These sorts of physical gestures are in line with the expectation of Natural User Interfaces (NUI) that the system can leverage the user's existing intuition about physical objects to guide their interaction with the screen. 

However, detecting and using these gestures is prone to some degree of technological difficulty. 
Gestures made off of the screen, in the space above it, would require additional sensing to detect, such as a depth camera pointed down at the screen or towards the user from the front (or both). 
These gestures could be mapped back into the space of the screen by flattening them to the areas that they occur over, but the use of a sensed volume could also provide the path towards operating a UAV swarm in a direct manner. 
The user could use free space gestures to define boundaries for the swarm, or push the swarm around directly. 
However, such a user interface does not provide feedback in the same space as the gestures. 
In the user interface proposed for the gesture elicitation experiment, the response to a 2D gesture occurs on a 2D surface, and is a projection of a less-than-3D area onto that 2D screen.
The reason that the area is less than 3D is that while the terrain may have elevation variation, the robots do not leave the terrain. 
As a result, the user does not have to mentally ``reproject'' the 2D space into 3D in order to understand the interface. 
For a swarm of UAVs, the display would have to do as much of this work as possible, to limit the mental labor performed by the user. 
For example, the UAVs could be displayed at a size related to their distance from the viewport provided by the screen, using perspective to indicate distance. 
The viewport might also need to be movable, as the user may wish to view the shape of the swarm from multiple angles. 

In addition to the possible issues with gestures in space over the screen, the technology of the multitouch interface can constrain the available gestures. 
The Microsoft Surface tracks contacts as areas with a shape, centroid, and so forth. 
In software operating on the Surface, finger contact points are typically approximated as ovals, and distinguished from larger contact points such as the heel of the user's hand, by their size. 
As a result, if the user extends their fingers and places the edge of their hand on the screen, and then slides their hand along the screen, the Surface will report a single oval, much longer than it is wide, moving along the screen. 

The multitouch screen used in this work tracks up to 20 contact points, but it treats them as points, each representing a single pixel location on the screen. 
If the user preforms a sweep with the edge of their hand, the screen will report a large number of individual contacts in the region under the user's hand, rather than a single region of contact.
As a consequence, gestures that use a region of the hand other than the fingertip are difficult to recognize on the 3M screen. 

\section{Gesture Modification}

Participant actions in the formation task provided two interesting gesture commands that used alterations of previous commands to increase the versatility of formation commands. 
The first was indicating that the gesture was intended as a formation command by holding one finger down on the robots while drawing the formation with the other hand. 
This gesture arose from one participant's initial use of a single-finger drag to move the robots in the initial ``Move the robots to area A" task. 
For the line formation task, the participant initially used the same gesture, placing a finger on the robots and then dragging the line on the screen. 
The participant then recognized that a simple drag was ambiguous with the motion command, and so placed one finger from each hand on the robots, holding one still and dragging the other one out to indicate the line for the robots to form. 
The gesture was initially explained as ``pinning'' one end of the robots and ``stretching'' them out along the line.
On the next task, commanding the robots to form a square, the participant again placed both fingers on the screen, but drew a square rather than a line. 
This sequence of removing ambiguity and then extending from the disambiguated gesture resulted in a set of gestures that seems to compose a ``formation'' gesture (placing and holding a finger) with a description of what that formation should be. 
This gesture ended up not being used in the gesture set for the robot interface due to the fact that only one participant chose it, but in a context where training was available, it would be relatively simple to train users to perform this gesture. 

This sort of combined command-and-modification gesture would probably be easier to remember than sequences of gestures, as the sequence would consist of a larger count of discrete actions, rather than being experienced by the user as a single action. 
Additionally, the fact that the hold-and-draw formation command has one hand remaining still is likely a good idea from a design perspective. 
In the physiological research literature, tasks where each hand performs a different action are referred to as bimanual coordination tasks.
Bimanual tasks interfere with each other, as in the classic example of patting one's head with one hand and rubbing one's belly with the other. 
Interference between tasks of differing difficulty in a bimanual coordination task was seen in the case of aiming, where the hand making the easier motion was shown to slow to match the hand making the more difficult motion, rather than the more easily-completed motion being finished first \citep{fitts1954information}.
For an overview of the neurological basis of bimanual coordination and the various factors that influence interference between tasks, see \citep{swinnen2004two}. 
Having each hand performing a different gesture is cognitively demanding, compared to having one hand remain still or perform the same gesture as the other hand, so designs that allow one hand to remain still will be easier to use than those that require drawing different symbols with each hand, for example. 

Another participant gesture that made use of formations was the use of an adverb in the gesture language as described in the section ``User Strategies for Manipulation''. 
The participant used one finger to draw paths for the robots to follow, but specified that if two fingers were used to draw the path, the robots should follow the path while remaining in their current positions relative to each other. 
Modifying path drawing in this way allowed the participant to form the robots into a scoop-shaped formation, and then to move the scoop to position the crate in the crate manipulation tasks. 

As with the formation command example above, the gesture is an extension of a previous gesture, but with an addition to disambiguate it. 
These signals to the system that the command is being modified point to a commonality in the participants' thinking about the design of the gestures. 
Participants added additional contacts to a base gesture to indicate an extension of the base gesture to a special case. 
Holding one finger still changed path following into formation, and dragging two fingers changed path following into path following in formation. 
This combination of additional fingers with a base gesture suggests that other base gestures could be similarly extended with additional fingers to activate different, but related functionality. 
For example, some of the tasks displayed a mix of orange and red robots in one area (for an example, see Figure \ref{fig:10_orange_red_mix}), to attempt to elicit user gestures for sorting robots. 
A single-finger selection could select all robots, and adding fingers could be used as the extension to select each different color of robots in turn, with e.g. two fingers selecting the red robots and three fingers selecting the orange robots. 

\section{Gesture Direction}

Another area in which gestures may be treated with finer granularity is the direction in which the gestures are made. 
A few participants in the gesture elicitation experiment made verbal mention of this distinction, or used it in their control schemes. 
One participant used a clockwise lasso as a robot selection gesture, but a counterclockwise selection gesture to indicate dispersion. 
Another participant indicated that lasso selection should be done clockwise, while patrol areas were selected with a counterclockwise closed loop, similar to a lasso. 
The participant did not indicate how they would perform a patrol that moved in a clockwise direction around the patrol area, or if the direction that the gesture was made in was intended to constrain the patrol to move in the same direction as the gesture. 

Box selection also has the potential to be performed in four different orientations, with the gesture starting from the top left, top right, bottom left, or bottom right corners of the box. 
One participant made use of this distinction, comparing it to the box selection in the Autodesk Solidworks CAD software. 
Solidworks has two modes for box selection. 
If the mouse moves from left to right, the selection includes only objects that are contained entirely within the box. 
If the mouse moves from right to left, the selection includes all objects that the box overlaps to any degree. 
The participant used box selections starting from the top right to separate part of the robots from the others, and box selections starting from the bottom right to select only robots that had been marked as being in a particular group. 
Most other participants only used box selections starting from the top left of the box area, so for a UI supporting the distinction between the different forms, starting from the top left should be the basic gesture, and box selections starting from the other corners of the box would be reserved for more specific forms of selection. 

It is worth noting that there are inherent physical limits on the methods of distinguishing gestures explored so far. Adding more fingers to a gesture is limited by the multitouch sensing technology and the user's supply of fingers. 
Lasso can only be performed in two different directions, and box select only has four different corners that the selection gesture can start from. 

\section{Gesture Velocity}

Another possible distinction between gestures is the speed with which the gesture is made. 
One participant in the gesture experiment made a distinction between dragging a robot, which is performed slowly, and flicking a robot (to remove it), which is performed rapidly, with the intent of metaphorically tossing the robot off the edge of the screen. 
It seems unwise to alter the meaning of a gesture based on its speed, as in the case of movement becoming deletion if performed quickly, because a user may accidentally perform a gesture too slowly or too quickly, and so obtain a different result than they expect. 
However, the velocity with which a movement gesture is performed might reasonably be used to convey to the robots that the motion is to be performed quickly, while a slow gesture might indicate that the robots should proceed slowly.
Research would have to be performed to ensure that the increase in velocity does not result in a loss of accuracy beyond what the user is willing to accept.
  
Scaling of the velocity from the user commands to the robot commands also presents a possible issue, as the user interaction point may traverse the map more quickly than robots can traverse the real terrain. 
In a relatively extreme case, if a map of the continental United States is displayed on a typical touchscreen, the user could easily swipe their finger from one coast to the other in a few seconds. 
No practical mobility platform can be expected to match this velocity. 
As a result, the robot speed and user gesture speed may be related in a potentially complex way by other factors, such as the scale of the map view, leading to difficulty for the user in understanding how fast the robots are being commanded to move. 

\section{Assessment of Training-Oriented Gestures}

If the system were developed with this style of gesture set, assessment of the validity of the design choices would differ from the assessment of a system based on gestures intended to be useful to untrained users. 

In the case of a system intended to be useful to untrained users, the system's primary metric of effectiveness would be the ability of untrained users to complete tasks at all. 
In the experimental study, some users performed the formation task by repeating the single-robot motion gesture to move each individual robot into a position on the formation. 
This control scheme would allow the user to complete the task, assuming the single robot motion gesture was supported, but it is relatively inefficient compared to using a formation command button once. 
Many of the other tasks could be performed in this way, as sequences of single robot move actions, so as long as the user discovered that interaction method, the system could be used, albeit slowly. 

For a system that permits a longer training period, factors such as efficiency and memorability of the commands would be important, as well as the length of time to become proficient. 
Efficiency could be quantified as gestures required to perform a task, with a lower gesture count indicating a higher efficiency.
To return to the previous formation example, moving ten robots into formation with single robot motions requires ten gestures, while using a formation button requires a single selection gesture, a button click, and a single gesture to draw the formation, for a total of three user interface interactions.  

Testing the memorability of the system would require testing of the same user population after training, and again after a period of time not using the interface, to determine how many of the gestures were learned during the training, and how well they were retained over the gap of non-interaction time. 
Memorability interacts with training time, as it would be expected that a more memorable gesture set would allow users to become proficient more quickly than one that is difficult to remember.
One factor that may affect the memorability of the instructions is similarity across modifications of gestures. 
For example, if a two-finger box select selects only robots of a specific color, a two-finger lasso selection should also only select robots of a specific color. 
As a result, the user can learn that two fingers is the modification that makes the action color-specific, and combine that modification with basic gestures to create more precise gestures. 
To extend that example, use of two fingers in a formation gesture might indicate that only robots of a specific color participate in the formation. 
However, enforcing this kind of consistency may result in implementation of unneeded gestures that complicate the gesture space. 
For example, maintaining consistency may result in commands such as sending only robots of a specific color to move a box, when the main concern to the user is that the box be moved, not what color the robots doing the work are.  

\section{Missing Gestures}

Neither the consideration of ``superuser'' gestures nor the interface designed from naive user gestures considered certain types of user interaction. For example, the tasks specified in the experiment did not include gestures intended as commands to the user interface itself, such as commands to change the viewpoint from which the user was observing the robots. 
One participant did suggest the use of reverse pinch as a zoom gesture, as is frequently used on cell phones, for a task where the user had to interact with one robot in a group. 
By zooming in, the user expected to have that robot cover more of the view, and so be easier to interact with. 
The user interface also did not ask the user to alter the color of individual robots, despite having some situations where the robots were divided into groups by color. 

The absence of commands such as viewpoint changes and alteration of robot colors was intentional, so the user could have a feeling of interacting directly with the robots, rather than interacting with the interface. 
Adding a set of commands that are only to the interface, and not to the robots, creates a layer of intermediation. 
Adding a second set of commands also creates additional opportunities for ambiguity, as the commands to the user interface and to the robots must be separate, so that the interface can determine which commands influence it, and which commands are intended to be converted into programs for the robots. 
The conversion of user gestures to robot programs was of more interest for this work, but in the future it would be useful to determine what commands users might want to issue to the interface, and how they can be separated from commands to the robots. 

The tasks presented in the experiment were intended to have an overlap with the tasks from \citep{micire2010multi} for purposes of comparison, but also to have a reasonable degree of coverage of actions the user might want to have robots perform. 
However, there is no task for sensor overwatch, or any other task that requires that the robots be pointed in a certain direction. 
As a consequence, the gesture set is somewhat agnostic to gestures that would allow the user to specify a heading for the robots to face. 
It could be argued that in this particular experiment, as the user was told to assume that the robots could execute the task they were given, that the robots will end up pointing whatever direction they need to point to complete the task. 
As a result, if the task was sensor overwatch, the robots would be positioned such that their sensor fields overlapped the desired area.

This lacuna in the range of gestures elicited by the experiment points to a more interesting element of the experiment design for future research. 
Heading is a quality of an individual robot (although, clearly, a swarm can point to a common heading), and so heading gestures could be elicited in a single robot case. 
It may be useful to consider what actions can be reasonably regarded as single robot actions, such as facing a direction, turning in place, controlled movements, and so on, and compare the gestures selected to perform those actions on a single robot with the same actions, applied to a swarm. 
In addition to providing a very fine-grained treatment of control gestures, the extension of the gestures to swarms may reveal interesting qualities of the participants' thinking about the swarm. 
For example, if a swarm is commanded to rotate in place, do users expect each robot to move on an arc around the centroid of the swarm, or each robot to rotate in place to a new heading?
In either case, how is the expectation reflected in the gesture used?
Based on the results of this work, particularly in the unknown number of robots or ``cloud'' case, the participants' understanding of the swarm as a single entity would likely override their thinking of it as a large number of individuals, and so they would likely expect the swarm to reorient with some robots moving on arcs around the centroid of the swarm, rather than each robot rotating in place. 

\section{Gesture Coverage}

Related to missing gestures, that is to say, gestures that were not elicited by the design of the experiment, is the question of gesture coverage. 
Because  there was no constraint on the gestures the users could choose, all of the users performed (or described) some form of interaction for each task they were presented with. 
There was no case where a user was presented with a task, but could not complete it because they did not have a gesture available, because any gesture they could imagine and physically perform was available. 

However, once the user gestures were codified into an interface, there is a limitation on which gestures are acceptable and which are not. 
As a consequence, a situation may arise where the available gestures in the interface do not provide the user with a way to complete the task. 
In this case, the coverage would be incomplete: the set of gestures does not cover all the possible tasks. 

Determining that all possible tasks are covered by a fixed set of gestures may not be a useful problem to attempt to solve. 
Without a method of determining the likelihood of each situation, and the cost of failing to respond effectively to it with the given interface, significant effort would be expended on situations that my simply never arise. 
For a real-world SAR application, a preferable approach may be to provide a set of gestures covering a range of abstractnesses, with simple motion to a point at one end, and more complex behaviors such as formations, specialized forms of dispersion, and area searches at the other end. 
By providing different granularities of control in this manner, the user can use the more complex behaviors when they recognize a situation in which such behavior is useful, and attempt to cross gaps in the gesture coverage by combining the simple behaviors. 
Some hope for the validity of this approach is to be found in the use of single robot movement gestures, repeated for each robot, to send the robots to formation by some participants in the 10 robot case. 
Rather than suggest a ``formation'' gesture, these participants combined gestures they had already created for motion to a point, and used them to perform the formation task. 
In the case of a gap in the gesture coverage, similar ingenuity and the availability of relatively fine-grained basic actions may allow users to complete the task despite the lack of coverage. 
However, some care must be taken to ensure that the basic actions scale well with the swarm size. 
The use of single robot moves to enter formation is possible with more than ten robots, but becomes prohibitively tedious. 