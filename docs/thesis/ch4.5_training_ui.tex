% !TeX root = ams_thesis.tex
\chapter{UI Design for High Training}
\thispagestyle{fancy}

The work that this study built on was motivated in part by the idea of using robots in search and rescue (SAR) operations, to explore damaged buildings, enter unsafe areas, and provide additional senses and coverage while reducing risk to first responders \citep{micire2010multi}.
First responders are trained on the equipment they use, but such training may be infrequent, and so complex control systems may not be remembered when the time comes to use the equipment. 
Because a SAR situation may develop rapidly, there is little or no time available for retraining, and so use with first responders prioritizes easy learning of the interface and use with minimal training over depth of the possible commands.  

However, SAR is not the only domain in which swarm robots might be used, and so considering only the gestures which maximize the ability of untrained users ignores domains where it is possible to train the users extensively. 
In such a context, adding complexity to the gesture set could allow for additional expressiveness in the interface.


\section{On-line Training}

Because user experience of video games does have some effect on their use of the user interface described in this work, it is interesting to examine other aspects of video games, and how they might be applied to user interface design for swarm robotics. 

It has been argued that the use of game-like user interfaces is inappropriate for the design of applications, but better suited to training material\citep{thomas1994games}. The basis for using a game-like user interface for an application is that games are engaging, and as a result, using a similar interaction paradigm should yield an engaging application. 
However, the dissimilarities in user motivation make this approach less useful. 
Games are played for the sake of their own play, rather than to accomplish a task outside of the game. 
Challenge and complexity are controlled in the game to make the play more rewarding, but extending this to applications for external tasks means making the tasks more difficult than they have to be, which will be a difficult proposition for end users to accept. 
Additionally, work and play are culturally strongly divided, and making one like the other will be resisted by users. 
Thomas instead proposes that computer-based training materials are a better target for game-like user interfaces. 
Like games, training materials are intended for high involvement over short periods, and are interacted with for themselves, rather than as part of the completion of an external task. 

The majority of the work on video games and learning is centered around the outcomes of games developed to be educational, rather than on how games that are intended to be purely entertaining still educate their users in the use of the game itself.
Modern games frequently include a tutorial level at the beginning of the game in order to familiarize the user with the controls of the game. 
The tutorial level provides instruction for the user in performing game actions, and assesses whether those actions have been performed, in order to control progression to the next section of the tutorial. 
For example, a game that includes an element where the player rides a horse may provide a horse, and instruct the user in how to use the controls to mount the horse and direct its movement. 
However, games do not always provide tutorials, and the lack of tutorial content, or the low quality of tutorials when present has been cited as a potential usability issue in games \citep{pinelle2008heuristic}.

The tutorial level of video games is usually somewhat separate from the main content of the game, in that actions in the tutorial have limited or no consequences in future gameplay. 
This raises a potential issue for implementation in a robot control system. 
If the system allows untrained users to control real robots in a tutorial mode, the user may make an error that does have permanent consequences for the future use of the system, such as causing a collision between robots. 
If, instead, the system allows the user to operate a simulation, the design and development overhead of implementing the tutorial is quite large, for a component of the system that the user may use very rarely or never. 
This dilemma may restrict pre-use tutorials to training the user as to which gestures the system can recognize, and only permit a sketch of the possible robot reactions to those gestures. 

Alternatively, the system could be developed with a basic level of functionality based on user elicited gestures, as in this work, and attempt to recognize certain relatively inefficient patterns of user activity. 
When such a pattern is recognized, the system would propose alternatives that extend beyond the easily-discovered gestures to offer greater finesse or complexity. 
For example, in the user experiment, some participants performed the Move to Area A task in the 10 robot case by selecting each robot in turn and dragging it to area A, while other participants made a selection gesture and then a single motion gesture. 
If the system detects a repeated pattern of single robot interactions, it could display a hint to the user that selection of multiple robots can be done, and indicate how to use it. 
In order to minimize annoyance, the user should be able to dismiss the notification, or prevent future notifications if they feel that they are sufficiently proficient in the use of the interface. 

The data set collected in the user gesture experiment provides an interesting sample of possible advanced gestures. 
Rather than being common across a large percentage of the users, these were gestures that a small number of users selected, in an attempt to provide additional expressive power. 
The rest of this chapter discusses the ``long tail'' of user gestures, and their possible use as ``superuser'' gestures after an appropriate training period. 

\section{``Other'' Gestures}

The ``other'' gestures are those that did not match any of the other codes as described in Appendix \ref{chapter:coding_defs}. 
The largest category of other gestures is gestures that were some form of ``sweeping'' the robots around, but because of the diversity of these gestures, there was little agreement from user to user. 
Some users made gestures over the screen, as if sinking their fingers into sand or a pile of small objects and dividing the pile, or cupping their hands to bring the pile back together. 
Others used the edge of a finger (typically the index finger) or the heel of the flat hand to push robots, as if they were sweeping them along a countertop. 
These sorts of physical gestures are in line with the expectation of NUI that the system can leverage the user's existing intuition about physical objects to guide their interaction with the screen. 

However, these gestures are prone to some degree of technological difficulty. 
Gestures made off of the screen, in the space above it, would require additional sensing to detect, such as a depth camera pointed down at the screen or towards the user from the front (or both). 
These gestures could be mapped back into the space of the screen by flattening them to the areas that they occur over, but the use of a sensed volume could also provide the path towards operating a UAV swarm in a direct manner. 
The user could use free space gestures to define boundaries for the swarm, or push the swarm around directly. 
However, such a user interface does not provide feedback in the same space as the gestures. 
In the user interface proposed for the gesture elicitation experiment, the response to a 2D gesture occurs on a 2D surface, and is a projection of a less-than-3D area onto that 2D screen.
The reason that the area is less than 3D is that while the terrain may have elevation variation, the robots do not leave the terrain. 
As a result, the work the user has to do to ``reproject'' the 2D space into 3D, in order to understand it, is limited. 
For a swarm of UAVs, the display would have to do as much of this work as possible, to limit the mental labor performed by the user. 
For example, the UAVs could be displayed at a size related to their distance from the viewport provided by the screen, using perspective to indicate distance. 
The viewport might also need to be movable, as the user may wish to view the shape of the swarm from multiple angles. 

In addition to the possible issues with gestures in space over the screen, the technology of the multitouch interface can constrain the available gestures. 
The Microsoft Surface tracks contacts as areas with a shape, centroid, and so forth. 
In software operating on the Surface, finger contact points are typically approximated as ovals, and distinguished from larger contact points such as the heel of the user's hand, by their size. 
The multitouch screen used in this work tracks up to 20 contact points, but it treats them as points, each representing a single pixel location on the screen. 
As a result, if the user extends their fingers and places the edge of their hand on the screen, and then slides their hand along the screen, the Surface will report a single oval, much longer than it is wide, moving along the screen. 
The 3M Multitouch screen reports a large number of individual contacts in the region under the user's hand, rather than a region.
As a consequence, gestures that use a region of the hand other than the fingertip are difficult to recognize on the 3M screen. 

The use of formation provided two interesting gesture commands that used alterations of previous commands to increase the versatility of formation commands. 
The first was indicating that the gesture was intended as a formation command by holding one finger down on the robots while drawing the formation with the other hand. 
This gesture arose from one participant's initial use of a single-finger drag to move the robots in the initial ``Move the robots to area A" task. 
For the line formation task, the participant initially used the same gesture, placing a finger on the robots and then dragging the line on the screen. 
The participant then recognized that a simple drag was ambiguous with the motion command, and so placed one finger from each hand on the robots, holding one still and dragging the other one out to indicate the line for the robots to form. 
The gesture was initially explained as ``pinning'' one end of the robots and ``stretching'' them out along the line.
On the next task, commanding the robots to form a square, the participant again placed both fingers on the screen, but drew a square rather than a line. 
This sequence of removing ambiguity and then extending from the disambiguated gesture resulted in a set of gestures that seems to compose a ``formation'' gesture (placing and holding a finger) with a description of what that formation should be. 
This gesture ended up not being used in the gesture set for the robot interface due to the fact that only one participant chose it, but in a context where training was available, it would be relatively simple to train users to perform. 

However, this sort of composability has limits. 
Sequences of gestures would probably be harder to remember than this sort of combined command-and-modification gesture, as the sequence would consist of a larger count of discrete actions, rather than being experienced by the user as a single action. 
Additionally, the fact that the hold-and-draw formation command has one hand remaining still is likely a good idea from a design perspective. 
In the physiological research literature, tasks where each hand performs a different action are referred to as bimanual coordination tasks.
Bimanual tasks interfere with each other, as in the classic example of patting one's head with one hand and rubbing one's belly with the other. 
Interference between tasks of differing difficulty in a bimanual coordination task was seen in the case of aiming, where the hand making the easier motion was shown to slow to match the hand making the more difficult motion, rather than the more easily-completed motion being finished first \citep{fitts1954information}.
For an overview of the neurological basis of bimanual coordination and the various factors that influence interference between tasks, see \citep{swinnen2004two}. 
Having each hand performing a different gesture is cognitively demanding, compared to having one hand remain still or perform the same gesture as the other hand, so designs that allow one hand to remain still will be easier to use than those that require e.g. drawing different symbols with each hand. 

Another participant gesture that made use of formations was the use of an adverb in the gesture language as described in Section \ref{sec:user_manipulation}. 
The participant in that section used one finger to draw paths for the robots to follow, but specified that if two fingers were used to draw the path, the robots should follow the path while remaining in their current positions relative to each other. 
Modifying path drawing in this way allowed the participant to form the robots into a scoop-shaped formation, and then move the scoop to position the crate in the crate manipulation tasks. 

As with the formation command example above, the gesture is an extension of a previous gesture, but with an addition to disambiguate it. 
These signals to the system that the command is being modified point to a commonality in the participants' thinking about the design of the gestures. 
Participants added additional contacts to a base gesture to indicate an extension of the base gesture to a special case. 
Holding one finger still changed path following into formation, and dragging two fingers changed path following into path following in formation. 
This combination of additional fingers with a base gesture suggests that other base gestures could be similarly extended with additional fingers to activate different, but related functionality. 
For example, some of the tasks displayed a mix of orange and red robots in one area (for an example, see Figure \ref{fig:10_orange_red_mix}), to attempt to elicit user gestures for sorting robots. 
A single-finger selection could select all robots, and adding fingers could be used as the extension to select each different color of robots in turn, with e.g. two fingers selecting the red robots and three fingers selecting the orange robots. 

Another area in which gestures may be treated with finer granularity is the direction in which the gestures are made. 
A few participants in the gesture elicitation experiment made verbal mention of this distinction, or used it in their control schemes. 
One participant used a clockwise lasso as a robot selection gesture, but a counterclockwise selection gesture to indicate dispersion. 
Another participant indicated that lasso selection should be done clockwise, while patrol areas were selected with a counterclockwise closed loop, similar to a lasso. 
The participant did not indicate how they would perform a patrol that moved in a clockwise direction around the patrol area, or if the direction that the gesture was made in was intended to constrain the patrol to move in the same direction as the gesture. 

Box selection also has the potential to be performed in four different orientations, with the gesture starting from the top left, top right, bottom left, or bottom right corners of the box. 
One participant made use of this distinction, comparing it to the box selection in the Autodesk Solidworks CAD software. 
Solidworks has two modes for box selection. 
If the mouse moves from left to right, the selection includes only objects that are contained entirely within the box. 
If the mouse moves from right to left, the selection includes all objects that the box overlaps to any degree. 
The participant used box selections starting from the top right to separate part of the robots from the others, and box selections starting from the bottom right to select only robots that had been marked as being in a particular group. 
Most other participants only used box selections starting from the top left of the box area, so for a UI supporting the distinction between the different forms, starting from the top left should be the basic gesture, and box selections starting from the other corners of the box would be reserved for more specific forms of selection. 

It is worth noting that there are inherent physical limits on the methods of distinguishing gestures explored so far. Adding more fingers to a gesture is limited by the multitouch sensing technology and the user's supply of fingers. 
Lasso can only be performed in two different directions, and box select only has four different corners the selection gesture can start from. 

Another possible distinction between gestures is the velocity with which the gesture is made. 
One participant in the gesture experiment made a distinction between dragging a robot, which is performed slowly, and flicking a robot (to remove it), which is performed rapidly, with the intent of metaphorically tossing the robot off the edge of the screen. 
It seems unwise to alter the meaning of a gesture based on its speed, as in the case of movement becoming deletion if performed quickly, because a user may accidentally perform a gesture too slowly or too quickly, and so obtain a different result than they expect. 
However, the velocity with which a movement gesture is performed might reasonably be used to convey to the robots that the motion is to be performed quickly, while a slow gesture might indicate that the robots should proceed slowly.
Research would have to be performed to ensure that the increase in velocity does not result in a loss of accuracy beyond what the user is willing to accept.  