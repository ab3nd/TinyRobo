\documentclass[]{article}

\usepackage{xargs} 
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

%opening
\title{Command Language for Single-User, Multi-Robot Search}
\author{Abraham Shultz}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

Methods for command and control that are based on issuing individual orders to individual actors, whether they are robotic or human, do not scale to large numbers of actors. 
In human organizations, this problem is solved by creating hierarchies of humans to propagate commands, with the attendant increase in cost and potential to introduce errors. 
By defining a mapping from user interface gestures to individual programs loaded on local robots, we can disintermediate the command structure, and allow an individual to control arbitrarily large, heterogeneous swarms.
In order to remain robust in the face of failure, the overall action of the swarm should be decentralized emergent behavior, rather than a centralized orchestration. 
Each robot receives its own program, and the sum of the execution of the programs on each robot results in task completion as emergent property.
Because a robotic swarm's situation develops over time, there is no such thing as a homogeneous swarm.
Each robot is at a distinct location and has a distinct level of resources such as remaining battery charge. 
As a consequence, robot selection for tasks in a heterogeneous swarm is best deferred until compile-time.

\section{Research Questions}

In order to create a mapping from a command language for multi-robot command and control, the command language must first be specified. 
The command language in this work is a system of common gestures as determined empirically in \cite{Micire:2009:ANG:1731903.1731912}. 
%Is there a coherent command language for controlling large swarms?  
For the purposes of this research, a coherent command language is one that is found to be intuitive by the majority of users. 
In \cite{Micire:2009:ANG:1731903.1731912}, for each available command, one or two gestures were used by \%60 of the users. 
In contrast, an incoherent language will have very little commonality of gestures used between users. 
Defining the scale required for a swarm to be considered ``large'' will be done empirically.
It is expected that there exists a transition point for the number of members in a swarm where users will stop interacting with the UI representation of the members of the swarm as individuals, and attempt to interact with the representations as groups or collections. 
For example, rather than selecting each robot by clicking on it, the may ``paint'' over the area containing the robots they want to use. 
A large swarm is, then, a swarm with a number of members above the point at which such a transition occurs. 

Once the language is defined, sequences of commands in that language must be translated into programs to execute on each member of the swarm. 
The program for each swarm member must be generated automatically, either by creation of a new program or by a synthetic approach from per-determined behavior primitives.
Because the motivating examples used in swarm robotic research are almost never related to swarm software development, the end user cannot be expected to do the programming themselves. 
All of the valid expressions possible in the command language should be converted into programs for the robots, or the user must be usefully informed as to why it was not possible. 
The synthesized program should result in convergence of the swarm's overall behavior to the desired result. 
Clearly, in a developing situation in the real world, success may be impossible, and so there is not a practical way to guarantee that a particular valid command sequence will result in a particular desired state of the world. 
However, certain minimum bounds on the problem may be able to be used to determine if a desired task is certain to fail.

Emergent behaviors arise from the interactions of actors with each other and the world around them. 
In the face of uncertainty in the world, the behaviors will also become uncertain. 
Programs synthesized to guide the swarm should be designed to be robust against failure or degradation of swarm members. 
The heterogeneity of the swarm may also be leveraged to increase its robustness against failures of individual nodes or alterations of the environment. 
Because heterogeneity increases the dimensionality of the solution space for program synthesis, it may adversely affect the performance of the program synthesis and the swarm's runtime convergence to the desired state.


\section{Work Plan}

\subsection{Build a swarm}

In order to experiment with swarm robotics, a swarm must be made.
Much can be done in simulation, but it is a daunting prospect to simulate the dynamics of real robot motion and the changes in the performance of each swarm member as mechanical wear and other forces influence them. 
Further, swarm simulations must not have bugs in the software which results in incorrect behavior. 
Genetic algorithms are infamous for exploiting quirks of their simulation environment which are not present in the real world, but still allow members of the population to inflate their fitness. 
Any behavior of a physical, unsimulated swarm is the real behavior of the swarm, and cannot be blamed on a defect in the simulation of the swarm or its environment. 

Because the focus of this work is to determine how gesture control scales or fails to scale, and humans primary mode of gesturing is with their hands, the size of the swarm should be significantly larger than the number of fingers a user could gesture with. 

\subsection{Swarm Robot Hardware}

Swarm robots are generally small. 
The reason to keep swarm robots small is two-fold. 
First, larger robots consume more materials per unit, and so costs more money.
As a result, for a given number of swarm units, larger robots will result in a higher cost swarm. 
Second, each robot requires some amount of space to move around in. 
To keep the ratio of free space to robots constant, the area of space used by the robots grows as the robots do. 
If the ratio isn't kept constant, the robots will crowd each other, and so large robots will require either a very large space, or become overly crowded.

The challenge of construction of swarm robots hardware, then, is to put all of the same parts as non-swarm mobile robots: a mobility platform, a processor, some sensors, and a communication system, into a small package.  
The robots described in this work accomplish this task by using a Commercial Off-The-Shelf (COTS) module to provide Wi-Fi connectivity and a microcontroller for processing. 
The module used in this swarm hardware is a ESP8266-03  \unsure{Get a picture of the ESP8266}, which provides a Wi-Fi interface and approximately 500kB of flash memory for programs. 
The ESP8266 is avialable in several form factors, each designated by a different suffix. 
The 03 version was chosen because it offers more GPIO pins than most other versions, and includes an internal antenna, which some versions lack. 

This swarm controller module was designed to be used as a replacement for the control electronics of children's toys, similar to the \unsure{Cite Ionas} swarm robots developed in PAPER. 
Most children's toys use either one motor with a mechanical linkage to cause the toy to turn when the motor is reversed, or two motors.
Two-motor toys frequently use either differential steering or have one motor provide drive power and the other provide steering. 

Unfortunately, such small processors do not have significant computational power. 
The majority of the processing will be performed on a host computer running the ROS software framework. 
However, the host computer will not be a central control node for the swarm. 
Instead, the host computer will include a separate process for each robot in the swarm, which will be allowed to drive only the robot that it is linked to. 
Each of these robot processes will effectively act as a local control program for the robot, but will have the full processing resources of the host computer. 
As a result, the individual robots can be small, lightweight, and relatively low power, but the system as a whole will endow them with significant computing power. 

Software for the swarm, then, will be of two classes. 
The first class is infrastructure software that provides things like a medium for communication between the robots, or emulating proximity sensing for each robot. 
Having communication mediated by software on the host will allow for simple experimentation with variable network bandwidth or reliability. 
The second class of software is the program that controls each robot. 
The system as a whole will not constrain the robot control programs to be identical across all robots, but it may be useful to constrain the robot control programs to be the same for certain experiments. 

Currently existing swarm robots are too expensive to build a large swarm, with the exception of the Harvard Kilobots. 
The Harvard Kilobots do not support hardware heterogeneity, but in real world applications, robots will become heterogeneous by interaction with the world. 
Wear and tear on the robots will result in differences between the robots, even if they had started from identical conditions. 
Even sending part of a swarm up a hill while the rest remain at the bottom will result in the higher members of the swarm having less battery life than those that didn't move. 
Battery life may be relevant to allocation of tasks, meaning that the physical elevation gradient of the robots becomes a gradient of fitness for a task as well. 

However, heterogeneity can also be leveraged to increase the robustness of a swarm and reduce its cost. 
A hetergeneous swarm containing both flying and ground robots can allow specialization of the individual components. 
By way of historical analogy, airplanes and cars have been separate types of vehicles, each specialized for a particular task. 
Flying cars, however, are normally either poor cars, poor planes, or both, because the demands of each task are different. 
Cars require a certain amount of weight and downward force for traction, but weight and downward air forces are the enemy of developing a good airplane. 
Rather than requiring that all robots be able to traverse all terrains, the ground robots can be developed to cover ground well, while the aerial robots are optimized for longer ranges or dwell times. 
Heterogeneity also allows the robots to be cheaper, because not every robot has to do everything.
Combining robots that can fly and robots that can roll allows a mixture of overview sensing and mission endurance that would be difficult to produce robustly and inexpensively with robots that can do both. 

In order to be both heterogeneous and inexpensive, the robots used for this work will be constructed by developing a consistent control hardware platform that can be attached to children's toys. 
Each controller will use a ESP-8266 wifi module for wireless communication and as a micro controller. 
The ESP-8266 costs approximately \$3-5, and contains both a wireless interface and a micro controller that can be programmed from a variety of programming environments and languages, including Lua and the Arduino variant of C/C++. 

In order to locate the robots within the experiment area, an overhead camera is used to detect QR codes on each robot. 
The QR codes provide location, heading, and a unique identifier for each robot. 
In this system, the central computer provides virtualization of the processing on each robot. 
Virtualizing the computational resources on each robot effectively allows us to borrow robots from the future. 
As technology develops, higher powered computers and and more capable sensors have become cheaper and smaller. 
Allowing each swarm node to have a virtual computer ``riding'' on it gives us as much computational power as we want at each point in the swarm, even if hardware of that power level is not currently available.
The robots themselves are effectively Internet of Things (IoT) nodes with the ability to move around. 
This is, then, an extension of the work done with mobile sensor motes in PAPER

The decision to use ESP-8266 wifi modules as the core of the swarm robots designed in this work is an extension of a long legacy of previous robotics projects. 

%detail the other projects here, chronologically
The use of COTS hardware in research robotics has lead to at least two platforms refered to as COTSBots.
Bergbreiter's COTSBots used mote hardware for the communications link and sensing, plus a motor control add-on board  \cite{bergbreiter2003cotsbots}. 
The mobility platform is a hacked toy, in particular, a specific brand of high-quality micro RC car.
At the time of this writing, the particular car used is moderately expensive for a toy car, although quite cheap for a research robot, costing a over \$100USD per unit. 
Bergbreiter's COTSBots use TinyOS, a modular and event-driven framework for developing node software. 
ROS also provides a modular, event-driven framework, so many of the design ideas, if not the code, are equally applicable to the system described in this paper. 
TinyOS is written in a dialect of C called nesC rather than ROS's polyglot approach. The motor and mote boards communicate using a messaging layer, again like ROS. 
The motor driver board is not commercially available, but can be custom-built by board fabrication companies, without the researcher having to assemble it by hand. 
Ohio State also developed a very small microwave RADAR that can go on the boards.

The second version of COTSBots arrived 8 years later, in ``COTSBots: Computationally Powerful, Low-Cost Robots for Computer Science Curriculums" \cite{soule2011cotsbots}. Soule and Heckendorne describe a platform composed of a laptop, which controls a modified RC car, tank, or similar toy through some combination of motor drivers. 
Due to the diversity of possible combinations of hardware that can be assembled into this configuration, it is still a very viable platform. 
However, the minimum size of this style of COTSBot is the size of a laptop, which is in turn dictated by the minimum size of a useful keyboard. 
The large size of these COTSBots demands a very large space if the density of robots in a large swarm is to be kept low. 
Additionally, each laptop has a screen, keyboard, and so forth that are not useful while the robot is operating. 
All of these parts add to the overall cost of the swarm. 

Many impressive designs for swarm robot platforms have been proposed, and constructed, but are no longer easily commercially available, or never were. 
Alice, by Caprari et al. packed a PIC16F84 processor, motors, RF and IR networking, and enough battery power for 10 hours of autonomy into a robot measuring under one cubic inch \cite{caprari1998autonomous}. Unfortunately, the processor is anemic by modern standards, and the platform as a whole is not commercially available anymore.

Work such as ``Development of a Miniature Robot for Swarm Robotic Application" updates the processor at the heart of a robot similar to Alice, called AmIR, but there is no evidence that AmIR was ever widely available\cite{arvin2009development}. Similarly, the robot described in ``A Miniature Mobile Robot With a Color Stereo Camera System for Swarm Robotics Research'' combines a relatively modern microprocessor with a DSP for on-board vision processing \cite{haverinen2005miniature}. Again, it is not a platform that other researchers could buy.

The Jasmine swarm robots were possibly the closest thing to a successor to Alice.
Jasmine measured 26x26x20mm, and included an ATMega processor, IR close range communication and obstacle detection, two motor skid steering, and li-po batteries.
Jasmine V was intended as of 2007 to include aggregation/modular robot functionality.  Unfortunately, Jasmine units cost about 100 Euro each when they were available. 
Plans and information to reproduce Jasmine are available, but the chassis of Jasmine is a custom mechanical assembly, rather than a commercially available product. 
This work intends to demonstrate that modified toys are an adequate substitute for custom mechanical assemblies, and permit easy experimentation with heterogeneous swarms. 

The Epuck from EFPL is approximately ~800 swiss francs per unit, so the cost of maintaining a large swarm can become daunting quickly. Michael Bonani's MarXbot runs into a similar problem, in that it has a strong computer and a rich set of sensors and effectors, but as a result it is quite expensive \cite{bonani2010marxbot}.

Clearly, it cannot be expected of all swarm robotics researchers that they start and maintain a side business supporting and selling robots (much as the author might appreciate it).
However, in the interest of allowing easy reproduction of research work, and extension of swarm platforms, it might be wished that all of the information required to reproduce swarm hardware platforms be made available in an easily-reproduced form.
One collateral goal of the work outlined in this paper is to produce a well-documented reference design that can be implemented by a researcher using common tools, and possessed of no more than hobby-level familiarity with hardware, and common tools. 
If researchers are not expected to become hardware entrepreneurs, they should also not be expected to become expert machine tool operators. 

%synthesis of other projects
The previously described projects tend to fall into one of two groups, from a hardware perspective. The first group uses microcontrollers and very limited onboard computation, but is small and relatively cheap. Due to their limited computation, these systems do not support complex algorithms such as vision processing. 
All of these control systems that involve a toy plus some control hardware don't really go into how the system deals with running more advanced algorithms on the robots (because they can't, the hardware doesn't support it), or with making and operating a large cluster of them (100 \$500 laptops on RC cars is still expensive). The software of my system is the interesting part, as it allows partitioning the power of a single large computer over a lot of ``virtual'' smaller computers, while still having electromechanical systems in the real world, and all the detail that that brings.

%my improvements

``Occlusion-Based Cooperative Transport with a Swarm of Miniature Mobile Robots" Jianing Chen et al. If you can't see the goal because the object is blocking your view, push on the object. 

$\Rightarrow$ What if there isn't a command language that makes sense for a 1-2 dozen robot swarm?

``Kilobot: A low cost robot with scalable operations designed for collective behaviors" \cite{rubenstein2014kilobot} Algorithms designed for hundreds of robots aren't impressive on tens of robots. Analogies with ants, thousand or more robots for research. All operations have to be constant time, anything that scales with the number of robots will take too long. Slip-stick vibration motion, rather than wheels/legs/whegs, etc. No real odometry. No motion of anything rougher than a table. No direction sensing, only range. ``Off'' mode is actually very low power sleep with a wakeup to check for the ``on'' signal every few seconds. Infrared beaming of programs with CRC for error correction. 

``The I-SWARM Project: Intelligent Small World Autonomous Robots for Micro-manipulation" \cite{seyfried2005swarm} Aiming for 2x2x1mm volume. Generally describes the challenges, but not how to deal with them. Indicates that an XML behavior spec language was created, but not what it's called or how it works. Iswarm was eventually created, using piezo actuators, about the size of a ladybug. Not commercially available. 






Obstacles via drawing lines on the floor of the robot enclosure. Can use color detection to create obstacles, walls, etc. 

Write ROS controller to learn control of heterogeneous swarm.
Learning algorithm for controlling heterogeneous robots. 
Each robot has an ID. Send a move command to an ID, observe displacement of robot. 
By varying motion commands and observing displacement, can figure out ways to get a robot to move to a point, even if the individual robot is unreliable, has slip in motion. 
Should also be able to quantify swarm members based on some criteria or other, e.g. speed. This will come in handy for action assignment.

ROS software to command robots

ROS networking emulation, with dynamically variable reliability
Implementation of RSSI for swarm based on location of robots, location of walls
Write ROS nodes to emulate inter-bot communication

Dynamic heterogeneity by simulating bot properties such as damage (e.g. one motor fails, reduced speed, sensor is failing/failed) and per-bot health and settings (e.g. battery charge level, sensors equipped, localization, etc).
Robots don't actually have all of these things, but by faking them in a modular way, we can answer questions about performance as system degrades.

\subsection{UI Designs}

The current state of the art in design for a decentralized swarm controller appears to be one of two methods. (TODO define why I'm not doing centralized control)
The first method is to come up with behavior primitives that can be performed as a function of local sensing by each member of the swarm. 
These primitives are then composed to come up with a program that the programmer thinks is likely to result in the desired behavior. 
The program is then tested and modified based on the result of the testing in an iterative fashion to converge on the desired behavior. 
Genetic algorithms and similar approaches have been explored for automating the iterative development of 


``World Embedded Interfaces for HRI". \cite{Daily:2003:WEI:820752.821587} 
 For very large numbers of robots, anything that depends on unique identities for each robot won't scale. Centralizing control or representing data won't translate easily into something humans can deal with. Neat "Jar full of little robots" image. Local information as gradients, overlaid on real-world with e.g. augmented reality. Robots as pixels, active fiducials (e.g. bokodes). Directional beacons on robots so they can see which way a message came in (have seen this before in other papers). This is only an interface for output, does not send commands to the robots. Does raise the distinction between hybrid teams where the person is in the environment with the robots and robot-only teams where the person is elsewhere. 

``Speaking Swarmish: HRI Design for Large Swarms of Autonomous Mobile Robots" \cite{mclurkin2006speaking} How to maintain, program, interact w/ robots without doing it on an individual robot basis. Autonomous charging is an enabler (my swarm doesn't have this...). Lights and sounds of robots as output channel. Again with the robot-as-pixel (which assumes you can look at them). Remote programming is key, they do it by having something remarkably like bittorrent across all the robots, updating in parallel. Something using TCP or UDP broadcast would be good. GUI "Inspired by real-time strategy video games". Colored LEDs on top of robots (~12 single-light patterns, several variations, 108 total patterns), audio system. Audio system plays MIDI, single notes; instrument, pitch, duration, volume are variable. Tempo and rhythm variation as code executes. Requires considerable user effort to ``play'' well. 

$\Rightarrow$ Look into real-time strategy games 

``Supervisory Control of Multiple Robots: Human-Performance Issues and User-Interface Design" \cite{chen2011supervisory} ``Call center approaches" to figure out what is going on, no SA at all prior to the call. ``Specialized interfaces and adaptive systems" will save us. Supervisory control is operating as the planner and monitor of the systems being supervised, but allowing the systems to operate on their own. Ten levels of automation, 10 is no-human-involved, 1 is no-automation-involved (Parasuraman's ranking system). ``Common ground" as in team play (Stubbs et. al) to facilitate interactions using a robot proxy (robot has a stand-in during the planning to enhance collaboration). High level of operation can leave operator out of practice, or encourage mis-placed trust in automation's ability. Operator multitasking starts to fall apart with more than about 16 entities being operated at once. This bodes ill for kilo-swarms. Management by consent and by exception could still generate too many events. 

Olsen \& Wood Fan-Out for UGVs around 5-9, depending on environment complexity. Again, bad for swarms. UAV and UGV together have reference frame issues (exo- vs. egocentric) when trying to combine incoming data to present to user. 

Suggestions for interface design. Automation behavior should be visible and explainable to user. User should be able to extract meaning from information display quickly (``Swarmish" and the robot-as-pixel designs). Projected changes and predictions should be highlighted so user understands consequences of their actions. Re-instruction of the system should be quick \& easy. This last point might be bad for a plan/code/compile/run kind of paradigm, because programming isn't fast. ``Interfaces... are efficient when they are compact and when they accentuate information that guides the decision making process" So part of making the interface good is making it good at deciding what to tell the user and what to hide. Multimodal (visual/audio, visual/haptic)interfaces are faster and result in perceived reduction in workload. Scheduling incoming information that the user has to deal with reduces workload bottlenecks. Indicating which robots are in a team with a semi-transparent form covering the team was slightly preferred by user to individual robots, in interfaces where all robots are depicted. 

Adaptive automation to level workload on user. Engaged at low load times, assisted at high load times. Adapt based on perceived load or physiological markers in the user. May not be relevant for fire-and-forget swarm missions. 

TeamTalk, TraderBot, and Plays - Task allocation and shared mental model work for human/robot teams. May also work for conveying the swarm model to the user so that they understand what is going on. $\Rightarrow$ Unsure if it can convey commands or queries back, but probably can. 

$\Rightarrow$ How is swarm performance on task evaluated? Progress towards completion measured how?

``Biologically-Inspired Human-Swarm Interaction Metrics.'' \cite{harriott2014biologically} Caroline E. Harriott, Adriane E. Seiffert, Sean T. Hayes, Julie A. Adams. (2014) How humans respond to interacting with swarms is not well known (and therefore a good thing to research). Nine categories of metrics, with examples, from biological and robotic swarm literature. Swarms have more uncertainty (individual reliability is low) and higher attentional demands (many individuals). Human interaction focus on macro level (cites another paper for this suggestion), influence of swarm actions rather than control of individuals. Some transition point between multi-agent and swarm, this paper puts it at ~50. 

Human attributes - how the human interacts, trust, intervention frequency, etc. \\
Task performance - ability to accomplish task, speed, accuracy, cost \\
Timing - Command diffusion lag, behavior convergence \\
Status - Condition of the swarm, battery life, number of functioning members, stragglers \\
Leadership - Interaction between special members of the swarm and others, how well leaders are followed \\
Decisions - How actions are taken, likelyhood that the correct action is chosen \\
Communication - Speed, range, network efficiency \\
Micromovements - Motion of individual swarm elements relative to each other \\
Macromovement - Motion of the overall swarm, flocking, elongation, forming fancy shapes \\

``Heuristic Evaluation of Swarm Metricsâ€™ Effectiveness'' \cite{manning2015heuristic} Matthew D. Manning et al, 2015. Visualization of swarms is gnarly. Base case is display all the units. Others include blob, blob with density shading and motion arrows, field of influence for leaders, web of information (data flow in the swarm). Lists a lot of metrics (diffusion, CoG, directional accuracy, flock thickness, resource depletion, swarm health) and assesses how well each view displayed it. Paper doesn't really list heuristics or methods, which makes me think they just kind of thought about it for a day or two and then wrote it up. Cites some interesting-looking papers in biology. 

A lot of Dr. Adams' work in HSI was under ONR Award N00014-12-1-0987

``Multi-Touch Interaction for Tasking Robots'' \cite{hayes2010multi} Sean T. Hayes, Eli R. Hooten, Julie A. Adams. (2010) Multi-touch better than WIMP, voice interface. ``The specific tasks are Go To location X, perform a Reconnaissance, and Cross a Dangerous Area (CDA)". ``Control widgets were placed on the map and are draggable or semi-transparent, allowing as much of the map to be displayed as possible" kind of like DREAM. Multi-touch interaction was pinch zoom and two-finger pan. Outlining regions with drawing gestures, paths with fluid strokes (not vertex selection). Faster task completion, especially for region selection. ``This work was partially supported by a contract from the US Marine Corps Systems Command to M2 Technologies, Inc." Basically, a lot like stuff our lab has found out, (hurf durf, because they cite Mark/Drury/Yanco on multi-touch-multi-robot). NASA TLX for human factors like frustration and workload. 

$\Rightarrow$ What about just dropping malfunctioning individuals from view? Fits with the assumption that they are expendable, and allows human to have a rough idea of how bad things are getting by watching cloud shrink. 


``Kinetics of Robotics: The Development of Universal Metrics in Robotic Swarms" \cite{jantz1997kinetics} Scott D. Jantz, Keith L. Doty, James A. Bagnell and Ivan R. Zapata (1997). Treats robots as a gas, each robot is a particle, calculate expected dynamics from that, and adding sensors and intelligence as improvements from the ideal gas state. Doesn't seem to deal explicitly with tasks, since a gas doesn't really perform tasks. 

$\Rightarrow$ Read up on ``Ecological interface designs'', Vincente 113 in ``Supervisory Control..." display constraints as emergent patterns so user can get an intuition for the solution? 

``Ecological Interface Design: Progress and Challenges" \cite{vicente2002ecological} Kim J. Vicente. EID is a framework for HCI for complex systems with a social component. As of writing of this paper (2k2), had not been tested in the real world. Automation takes over basic tasks, so worker has to deal with exceptions, must adapt to the unfamiliar in potentially high-stakes situation. "Reciprocity of user and environment, represenative design of experiments and evaluations, primacy of perception, and start with analyzing the environment". Information about a system is abstracted across a hierarchy, high level is functional information ("This plant makes reciprocating flange whompers"), low level is physical information ("This is a fleeble grommet, it is located in the engine room"). Hierarchy is over work domain, not task. EID should encourage skill and rule-based behavior (for normal operations) while also allowing knowledge-based behavior (to solve unanticipated problems). Interface should allow direct manipulation of perceptual forms that map directly onto work-domain constraints and represent all of the information (!) identified by the abstraction hierarchy. In a swarm context, this means displaying functional information across the hierarchy from swarm bots to high-level tasks so the user can interact with it if something goes wrong. Some way to represent and switch between robot tasks or behavior priorities would be key. 

``Ecological Interface Design: Theoretical Foundations" \cite{vicente1992ecological} Kim J. Vicente (1992) Early work in the field. Skills, Rules, Knowledge taxonomy, don't force processing to a higher level than task requires, but support all levels. Events are also in three levels: routine, foreseen exceptions, unforeseen exceptions. In order to control a process, the human/machine system must account for the complexity of the process and the constraints of the work domain. Abstraction hierarchy as above, high level is the functional, low level is how the function is accomplished. Detection of exception requires the display of all constraints, because exception is the breaking of constraints, and undisplayed constraints cannot be assessed to determine if they hold. 

$\Rightarrow$ Is trust even a factor with a swarm? Once some barrier is crossed (where is that point?) User may not care about individual units, so as long as progress appears to be being made on the mission, the user might let underperforming units slide. System might not even announce it if units are lost, until it starts to affect performance.  

$\Rightarrow$ What level is a swarm expected to operate at? At the swarm-as-a-whole level, it's more or less level 1, human tells swarm what to do. At the individual robot level, it's 7 or higher. 

$\Rightarrow$ Is it a valid assumption that the swarm is never teleoperated? What is the use case for being a single ant or bee, especially out of thousands of units?

$\Rightarrow$ Look into MiDAS (Mission Displays for Autonomous Systems). 

\subsection{Derive a gesture language}

\cite{Kato:2009:MIC:1520340.1520500} has a multi-touch top-down interface, user specifies vector field. Just intended for moving robots around. Focus on the vector field rather than individual robots, so this could be handy for e.g. gathering and dispersal. Makes assumptions (for display) about knowing all the locations of all of the robots that might not be available in a real situation. On the other hand, the exact locations of the robots don't need to be known if the display is e.g. cloud of robots. Vector field paths can't cross each other (they flow together), but the can have loops (which waypoints can't do, as they have explicit ends) None of this results in a program that works on the robot without communication, as the computer orders all the robots around based on a global knowledge (the field).


Do user study to determine if there is a control language for multi-touch interfaces for robots.
Method similar to Mark's work, but for way more robots than the user has fingers. 
Dozens to hundreds of robots. 
Same tasks or similar tasks? Same, but possibly with some extensions. 
High level tasks like searching an area, finding something. 

Determine if grammar exists. 

Is there a non-multitouch way to do this? Beyond a few robots, one-at-a-time selection doesn't scale. Need mass selection, maybe spreading from a point while the pointer is active?

Will users care about unused or underused robots? At what point, count-wise, does the user stop caring about individuals and start ordering the robots around as a cloud or flock? Does framing the question to the user in a specific way allow us to vary that parameter, so that by couching our questions in a certain way, we can decide what the user cares about and how they command the robots?

$\Rightarrow$ What about shaping and pushing the swarm the way kids will play with a bug, putting their hand down so the bug goes around or avoids it, touching the back of the bug gently to make it scurry forwards, etc? Shaping the group as if sculpting, with pushing and pinching to carry groups around?

Could have selection based on space-partitioning, so there is no way to leave a robot out because of the shape of a partition. E.g. split space in half, tap splits tapped half, etc. Select a region and go. 

If grammar exists, implement in simulation for e.g. multitouch

Implement for real robots (the little tanks, the swarm I'm making, Harvard kilobots)

If no grammar exists, why not?
How would we detect failure to converge to a grammar? No two users using the same commands, or very poor inter-user cohesion in the command sets. If no gestures are more likely than any others, then there is no preferred command set or gesture set. 
This is actually not much of a setback, as it lets me select a grammar and develop it; or select pair of grammars, and then compare them.

Another possibility, define a grammar like Tierra, where all gestures are valid commands

Probably worse than yelling at user over invalid commands, since they end up constantly sending the robots to do the wrong things

Allow users to operate swarm over internet, see what people try to use to control a swarm
Ends up causing issues because most users don't get to try (one user at a time) if it is commanding a real swarm. Swarms in simulation or web-based interactive pages don't have that problem. Can also record all the input. 
Can't possibly be multi-touch, as user hardware and OS support isn't really there. Most people won't do it if your instructions start with ``get a second mouse''. 

Could be multi-user with real robots or simulation, but runs into problems with arbitrating between which commands are used. Could do sum-of-vectors or something for shared control, like haptic/shared autonomy stuff. Could also just let users partition the robots by negotiation among themselves. Not sure how communincation for this would work. 

User interface allows the user to define an overall desired distribution of the robots, position and directions, and then the control software compiles that into a representation in e.g. GPL or OSL that gets compiled into programs to run on the robots. This isn't a continuous controller, more fire \& forget. Could have robots with a main behavior of successfully navigating the world and other behaviors (search and rescue, track target, etc.) as riders. Sort of like putting sensors on existing bees, and then collecting the data while they do bee stuff. 

\subsection{Conversion of user tasks into robot programs}

"Programming the swarm" \cite{evans2000programming} David Evans (Looks like a doctoral thesis proposal) Swarm as collection of mobile, physically situated processors communicating over an ad-hoc network. Behavior of swarm as emergent from behavior of units, resilient against "misbehaving members". References Amorphous Computing. Two approaches to programming: compose program from primitives, or synthesize unit programs based on description of environment and desired behavior. Analysis is approximate, rather than general proof of properties.

Compositional approach strikes me as something like abstracted Tierra, where any combination of behaviors is a valid program, but might not be a good/useful program. 

A lot of the applications listed in PtS are similar to foraging search. Spread into area, locate and converge on objective. Paper is mostly on lightweight formal methods and policy checking. Could be used at a policy level for a swarm, e.g. to limit overall power use, but adds another layer of behavior that can alter emergency. Lists as primitives: Disperse (no other nodes within distance d), general disperse (no more than n nodes within distance d), clump/cluster, attract to location, swarm in a direction, scan area, Broadcast/partialcast/unicast. Composition of programs as serial, parallel, divided, global (synchronized). I think the only composition that makes much sense is the one observed in biological organisms, that is, priority based parallel evaluation, where different behaviors become active as context requires. 

Mobile software specs: "Mobile UNITY [Roman97], the Distributed Join-Calculus [Fournet96], and Mobile Ambients [Cardelli98]."

"A much more ambitious goal is to produce the device programs for a primitive swarm program from the high-level behavior description. It is not realistic to hope that we could completely automate the process." This is pretty much what we're talking about doing. 

Lists tuple spaces, amorphous computing, as potential enabling technologies for swarm programming. 

"Continuous Space-Time Semantics Allow Adaptive Program Execution" \cite{bachrach2007continuous} Jonathan Bachrach, Jacob Beal, Takeshi Fujiwara. Spatial computer model, devices that interact depending on their location in space. Computer is "programm[ed]...as a continuious space". Swarm robotics as a special case of the spatial computer. "Proto" is language for a continuous plane spatial computer. Operations act as reduce() or similar over local area or history, feedback loops provide state. Paper provides emergent computation of firefly-style synchronization (coupled oscillators, so perhaps some elements of swarm computing can be expressed as variably-coupled oscillators in combination with driving signals derived from internal states and sensor inputs. This would in some ways be the ultimate evolution of the BEAM "Robot Jurassic Park" idea from the '90s, where each robot is sharing some of its internal state via coupled oscillation with the other robots in the park, e.g. to communicate the presence of light sources for ``food", the presence of self for collision avoidance, etc.)

"The mechanism for binding sensors to names is implementation dependent, as are the value when sense is applied to an unbound name and the result of multiple streams being sent to the same actuator." So, uh, the actual interaction of the program with the world is ``implementation dependent"? Good, knowing what your code does is for suckers. 

"Programming with Stigmergy: Using Swarms for Construction" \cite{mason2003programming} Zachary Mason. Homogeneous swarm with no memory, only local perception. A set of environmental triggers is "coherent if no stage in the building process can be confused with an earlier stage by making only local observations, thus obviating the need for centralized control". Agents move at random, actions governed by perception of environment at present time. Environment can include pheromones.  "Other future work includes programming construction swarms by specifying the target structure directly, letting a compiler infer the corresponding rule-set (if one exists)."

"A Formal Approach to Autonomic Systems Programming: The SCEL Language" \cite{nicola2014formal} ROCCO DE NICOLA et al. Self-managing autonomous ensembles of systems. Only related to swarm robotics in the sense that it is for more than one computer. Assumes complete localization and inter-swarm communication. 

"Scripting the Swarm: Event-based control of Microcontroller-based robots" \cite{magnenat2008scripting} Stephane Magenenat et al, EFPL. Introduces ASEBA, an event-based control architecture. Reflashing e-puck takes a minute, doesn't scale to a bunch of robots unless you can do it to all robots at once. Concurrent debugging is impossible. Programs compiled to bytecode, react to incoming sensor data (e.g. message from other robot, impending collision, etc.). Could compose swarm behavior by deciding reaction priorities, tuning priorities in reaction to environment. Still doesn't describe task-to-program compilation. Does permit differing code between robots, which won't scale to 1k+ robots. 

"Structure Synthesis on-the-fly in a Modular Robot" \cite{revzen2011structure} Shai Revzen et al. Robot uses binary foam to make structures, including bodies for other robots by sticking foam to robot parts. Not really a swarm thing, unless you count the other robot parts.  

"Automated Construction using Co-operating Biomemetic Robots" \cite{bowyer2000automated} Adrian Bower. Robots that build as an emergent property of individual behavior. Has a few basic rules to govern arch and column building. Mostly an exploration of potential ideas, without a lot of conclusions. Isocyanurate foam is awful stuff. 

"Collective Construction with Multiple Robots" \cite{wawerla2002collective} Jens Wawerla, Gawav S. Sukhatme, Maja J. Mataric. Behavior based, virtual sensing. Communication and memory, even minimal, will improve the efficency of the system, but are not required to complete the task. Task is implied by the behaviors, not compiled from a higher-level specification. 

"Emergence-Oriented Programming" \cite{palmer2005emergence} Daniel W. Palmer, Marc Kirschenbaum, Linda Seiter. Mentions GA to produce agent programs, could probably be used for developing behavior priority vectors. Hard to determine success, need to be able to recognize and evaluate performance on sub-problems. "The ideal way to program a swarm is to simply shout out high-level objectives and let the swarm figure out how to solve the problem." Meta-rules that govern selection of rules at the individual agent level depending on agent context. Nested hierarchies of systems and their contexts, but can be collapsed to an arbitrary level and its next level up (higher has no downward effect, lower can be considered part of base level). All information for the actor is local (its own sensors and memory, can possibly share with other actors). Uses aspect-oriented-programming (AoP) to recognize method invocations for single-agent actions as an aspect (e.g. maintain-distance and move-random forming a dispersion aspect). Analysis of the behavior of the swarm looking for good patterns and trying to develop code that does not yet exist to create behaviors that have not yet been observed is REALLY HARD, so have humans do it. This is a design process, not an automation process, but indicates a direction for possible ML approaches (which seems like a return to the GA approaches the authors dismissed earlier, possibly as less-directed than human intervention.). This requires multiple design/build/test loops for each activity the swarm tries to do. Perhaps have it learn forageForObject until object is found, surroundObject until object is surrounded, then "pushObject" until object is where it should be, then disperseRobots until the object is left alone again. Humans can do ``what-if" experiments on individual robots (so can the computer, and faster, assuming some evaluation function is available). Swarm algorithims extracted from Virtual Human Swarm (VHS) having people wander around a gymnasium. 

What is the existing work in compilation of programs for robots? 

Are there swarm-specific languages? 

"Proto" is a language for a continuous plane spatial computer. Proto paper also lists TinyDB as a "database view of the devices making up the computer", *LISP, and Regiment. Continuous time evolution approximated in gamma calculus and P-systems. 

"Infrastructure for Engineered Emergence on Sensor/Actuator Networks" \cite{beal2006infrastructure} Devices share internal state with local neighborhood, all run the same code bu can diverge due to neighborhood influences, sensor values. Proto is a stream-processing language, program is a directed acyclic graph with a single root (the output stream), composing programs is placing nodes into the graph. Proto is strongly typed, but with type inference rather than explicit/static typing. Doesn't cover conversion of a task into code. 

$\Rightarrow$ Look into *LISP, Regiment, gamma calculus, P-systems, Functional Reactive Programming, Gooze

"Combining Amorphous Computing and reactive agent-based systems: a paradigm for pervasive intelligence?" David Servat, Alexis Drogoul. Eventually everything will have an embedded, networked processor. Everything will continue to be gnarly: high heterogeneity in terms of software, processors, use cases, controllers and users. Things should still behave in a useful, coherent, predictable way. Requiring users to constantly manage them is not that. Amorphous computational model: computational units, which may be faulty, randomly located in a dynamic environment. Homogeneous programming, limited communication radius. Paper holds that most research in the area is computational models (e.g. ant colony optimization) or communications methods. Hop-count based gradient methods, bio-inspired based on quorum sensing. ``AC Hierarchy" for performing computing tasks ``simplifies programming with high-level abstractions". 

"Software Eng. For Self-Adaptive Systems" (Is a 270 page book, maybe skim later?)

"A Catalog of Biologically-inspired Primitives for Engineering Self-Organization" \cite{nagpal2004catalog} Radhika Nagpal.\\ 
"1. Morphogen gradients and positional information \\
2. Chemotaxis and directional information \\
3. Local inhibition and local competition \\
4. Lateral inhibition and spacing \\
5. Local monitoring \\
6. Quorum sensing and counting: \\
7. Checkpoints and consensus \\
8. Random exploration and selective stabilization:"\\
First five are common in amorphous computing, last three not so much (what are their uses?). Calls out gradient sensing in neuronal morphogenesis. Local inhibition and competition for symmetry-breaking. Quorum sensing to detect agent count required for a task (``do we have enough robots to move this box?") JIT allocation rather than pre-allocation is more robust, use the robots that are in the right place at the right time. Domino timing, where completion of each phase triggers the next. 

"Co-fields: Towards a Unifying Approach to the Engineering of Swarm Intelligent Systems" \cite{mamei2003co} Marco Mamei1, Franco Zambonelli. Again, the problem of robust software in dynamic environments with unreliable actors. Computational Fields (co-fields) are distributed data structures in a space, abstracting some features of the world perceived by the agents. Available as a dynamic systems formalism. Agents move on gradients in field spread by agents into the environment (great shades of pheromones, Batman!). Space doesn't have to be physical space. 

"Engineering Amorphous Computing Systems" \cite{nagpal2004engineering} Radhika Nagpal, Marco Mamei. Intro very similar to "A Catalog of Biologically-inspired Primitives for Engineering Self-Organization". Programming languages for AC: Growing Point Language (GPL, thanks, we needed some ambiguity) differentiates to form structures. Origami Shape Language, for dense agents on a deformable sheet. Describes local folds to eventually form an origami pattern. Both compile local program from overall description of desired result. Languages have different global properties. TOTA middleware provides fields and propagation rules for data, similar to co-fields. 

"Notes on Amorphous Computing" Jacob Katzenelson. \cite{katzenelson2000notes} Doing differentiation and integration on amorphous computers. 

$\Rightarrow$ Why is almost everyone ignoring power supplies except as an afterthought? Not a computer science problem?

$\Rightarrow$ Is it useful to share sensor information in a broadband rate-coded way, e.g. spike trains? Neuronal computation seems a bit like the limit case of amorphous computing, except that the network isn't radius-bound. 

$\Rightarrow$ Are there languages to describe a problem that is to be handled by a swarm, and compile to code for swarm members?

Assume the robots have a set of behaviors available (approach other robots, send a certain signal, move, avoid other robots, ``emit pheromone'') and can conditionally execute them e.g. when they see another robot, or when they reach a location. Is it possible to convert from user tasks to priority sequence or if-this-then-that style ``programs''?

Could be argued to be ``composition" from primitives.

``A Compositional Framework for Programming Stochastically Interacting Robots" \cite{napp2011compositional} Nils Napp \& Eric Klavins. Guarded Command Programming with Rates (GCPR). Robot behavior modeled as a stochastic process. Swarms are inherently concurrent (paper calls this the primary challenge, which I kind of doubt). Concerned with rigorous proof of correctness, which I maybe should be. ``Composed programs execute simultaneously". Assumption is that robots only have local sensing. GCPR guards are conditions on the environment, when condition is met, robot does actions at a given rate. This sounds a LOT like behavior-based robotics. In the concurrent case, this is modeled as each action happening one at a time, but in random order. Swarm behaves in a state space S, want to ensure that for all orderings of all actions, S ends in target final state. Physicality of system limits state transitions (robots can't occupy same space, can only move fixed distance in given time, etc.). Programs can be sped up or slowed down by scaling. Rate is typically the inverse of the mean time to complete the action (i.e. do things as fast as you can get them done). Markov semantics of the system description means robots randomly choose when to perform actions, once guards are met. By scaling time of actions, relative influence of actions is increased or decreased (e.g. if no obstacles move right with scale 4, move left with scale 2 means robots will move left or right, but twice as fast to the right, so robots will drift right overall). Failed behaviors are modeled as a program that doesn't do what the other program it is composed with does. Correct programs are those that reach the target state with probability one, even when composed with bounded failures (note that they might do it very slowly, but once they succeed, they stay successful).

$\Rightarrow$ In the real-world case of noisy or imperfect sensors, the variable time to execution of a guarded behavior would be caused by the imperfection of the robot's ability to detect that the guard was satisfied. In the real world, we get stochasticity for free. 

$\Rightarrow$ Get Klavins et al 2k4, apparently on algorithmic approach mapping local to complex global behavior.  

"Evolving Self-Organizing Behaviors for a Swarm-Bot" \cite{dorigo2004evolving} MARCO DORIGO AND VITO TRIANNI et al. The whole front page is authors. Swarm-bots assumed to be able to connect to each other, so this is a modular approach. Formation of an aggregate and motion of that aggregate. Evolution is often very slow, especially in physical systems, so simulator used. Evolve in simulation, test in hardware. Genotype is the weights of a single-layer perceptron that connects the sensors to the wheels. Evolved motion strategy figured out that spinning in place was the best move, which isn't what the users wanted. Aggregation evolves well, motion ends up being slow, and slowing as robot count increases beyond evolved values. A lot of the algorithims appeared to stablize (fitness-wise) after 25 or so generations. For mutual motion when linked, robots could sense traction, allows negotiation of a common direction of movement by proprioception. Can avoid obstacles and pull objects due to traction interactions. 

Didn't have enough robots to test in real life. Some decrease in performance when switching to a more accurate physical model for simulations. 

$\Rightarrow$ Quinn et all 2001, 2003, apparently did work on individuals (aclonal), rather than all robots having the same genome (clonal) Quinn, M. 2001a. A comparison of approaches to the evolution of homogeneous multi-robot teams. In Proceedings of the 2001 Congress on Evolutionary Computation (CEC2001), IEEE Press: Piscataway, NJ, pp. 128â€“135.
Quinn, M. 2001b. Evolving communication without dedicated communication channels. In Proceedings of the Sixth European Conference on Artificial Life, J. Kelemen and P. Sosik (Eds.), vol. 2159 of Lecture Notes in Computer Science, Springer-Verlag: Berlin, Germany, pp. 357â€“366.
Quinn, M., Smith, L., Mayley, G., and Husbands, P. 2003. Evolving controllers for a homogeneous system of physical robots: Structured cooperation with minimal sensors. Philosophical Transactions of the Royal Society of London, Series A: Mathematical, Physical and Engineering Sciences, 361:2321â€“ 2344.

"Evolving Aggrgation Behaviors for Swarm Robotic Systems: A Systematic Case Study" \cite{bahgecci2005evolving} Erkin Bahceci and Erol Sahin. From Mataric, evolution is only useful if it's faster than designing it by hand. Aggregation as a preliminary behavior, before doing something of interest (moving object, attacking en mass, etc. ). 

"Behavioral Feedback as a Catalyst for Emergence in Multi-Agent Systems" \cite{palmer2005behavioral} Daniel W. Palmer, Marc Kirschenbaum, LindaM. Seiter, Jason Shifflet, Peter Kovacina (2005). Not a lot of structure in swarm programming, lots of ad-hoccery, no automated agent program generation. Agents respond to emergent behavior in a sort of loop, or split-level hierarchy where the emergent behavior is the upper level and the agent behavior is the lower level. Basic swarm agents are assumed to be reactionary. Next level up uses local storage. Again with the AoP, behavioral feedback as an aspect. Used stochastic selection of actions, so kine of like Napp/Klavins work. 

What are the sorts of problems that can be solved by hardware-heterogeneous, code-homogeneous robots? what about het-het, homo-het, homo-homo? Does the distribution of hetero- or homogeneity affect the sort of problems that can be solved? Does it affect solution time or complexity? From a biomimetic standpoint, this seems very similar to asking if worker ants learn. If they learn, then they become hardware-homogeneous and software-heterogeneous, and can at least solve ant problems. If they don't, then they are homo-homo, and can still solve ant problems. Any problem that divides the robots into groups may be regarded as software heterogeneous, but could also be regarded as running different parts of the same ``if-else'' structure. 

A concept of the swarm as a whole as a programmable entity runs into trouble with reliability. In conventional compilation, assuming the compiler is correct and the computer is correct, the compiled binary does what the source code says. Robots interact with the real world, which is much less likely to be ``correct'' in the same sense a compiler can be asserted to be. Programs for swarms are only going to be functional within some probabilistic grounds and assumed conditions. 

How much of the behavior of the robots can be determined at compile time, and how much has to be determined at runtime?

Ideally, the robots can perform the task without requesting recompilation of the software running on each robot. 
This requirement indicates that the situation has to be at least somewhat known ahead of time, so that the robots will all receive programs that allow them to perform the task.
In the ideal case, the emergent action of all of the robots interacting with the environment causes them to perform the task. 

How can a user task get broken down into programs for individual robots?
Is it possible to leave room for emergency to solve problems in novel ways, or will everything have to be specified in predictable ways?

How are programs or behaviors for robots constructed? 
As a subset of this, how does varying percentage of a population that is running a certain program affect the emergent behavior of the system as a whole? 

How are individual robot actions parameterized? 
E.g. for clustering, range that robots can see each other, likelihood of a robot deciding to be a cluster center.  
Can variable parameterization be effective in the face of heterogeneous robot behavior?

Is it possible to build a compiler that converts user commands in task space into programs for robots without having to code in assumptions about the qualities of the robots? Does the compiler have to account for global properties (e.g. GPS denial)? Can the compiler account for robot-local properties (e.g. battery life, motor failure)?

How is task completion detected? Bio-inspiration would probably be something like quorum sensing or other local-communication-based saturation methods. Once all the robots are saying the task is locally completed, then the task is completed for at least the coverage area. 

``Stupid Robot Tricks: A Behavior-Based Distributed Algorithm Library for Programming Swarms of Robots", \cite{mclurkin2004stupid} James D. McLurkin. Set of comm techniques and library of behaviors for multiple robots in a group. Gradient-flood communications. Swarm behavior emergent from individual behaviors is sometimes unexpected. Library provides parts that interact predictably. Tested on 100 real robots. Was advised by Brooks, so there's the behavior-based robotics mentioned in the title. "Insects to not seem to have global names..." Neither do people (Eric, Eric...). Algorithimic state stored in the physical world, e.g. with stigmurgy or robot positions. Robots may not all be able to communicate with each other, network can be divided by distance, paper assumes they are all in one connected component (no way to know if they are not without known global IDs "is bob here?"). Robots might have global, local, or no IDs. Transmission of signals between robots is CSMA, can have collisions, limits throughput. Different types of gradient messages, with different propagation rules and rates. 

Behaviors ideally run concurrently, some respond to sensor inputs. IN the paper, the behaviors output is whether they are running, translational and rotational velocity, and LED configuration. Subsumption and summation are used to arbitrate between behaviors of differing priorities. 

Motion $\rightarrow$ moveArc, moveStop, moveForward, moveByRemoteControl, bumpMove

Orientation $\rightarrow$ orientForOrbit, orbitRobot, orientToRobot, matchHeadingToRobot, followRobot

Navigation $\rightarrow$ followTheLeader, orbitGroup, navigateGradient

Clustering $\rightarrow$ clusterOnSource, clusterWithBreadCrumbs, clusterIntoGroups

Dispersion $\rightarrow$ avoidRobot, avoidManyRobots, disperseFromSource, disperseFromLeaves, disperseUniformly

Utility $\rightarrow$ detectEdges

Demos ended up calling multiples of these. Motion specifications were (for individual robots) things like degrees of bearing and centimeters of proximity, so behaviors can be quantified. detectEdges is to detect if a robot is on the edge of the network. Behavior-based robotics explicitly doesn't learn, so ``bad environments" can defeat the swarm. Unfortunately, there's no such thing as a ``bad environment", just environments. Paper doesn't provide axioms for swarm programming at the group level. 

$\Rightarrow$ iRobot swarm had directional IR signaling (quadrants) using signal strength to figure out range and bearing, so that's something I'll want to include or emulate in my system. 

$\Rightarrow$ I should probably include some form of charging that doesn't involve a lot of hands-on interaction with my robots. 

``Pheremone Robotics" \cite{payton2001pheromone} Virtual pheremones with directional sensors. Largely a rehash of "World Embedded Interfaces". Modulated IR for sending messages. Hop-counting for diffusion. Budding with growth inhibition for exploration. Gas model as well, attraction balancing repulsion. Cites interesting work in directed diffusion, amorphous computing (processing with reaction-diffusion systems). "No explicit maps or models of env, no explicit knowledge of robot location". Walking in the world rather than walking in your head. 

``Compound Behaviors in Pheromone Robotics" \cite{payton2003compound} Compilation from this paper's ``set of logical primitives for controlling the flow of virtual pheromone messages" from some higher-level task representation may be useful. Virtual pheromones are local (no global information), diffusion-graded (spatial information), and decay over time (old information goes away). Can also contain other data, rather than just presence/absence. Paper describes set of primitives, composes gradient follow, go hide, and cooperative sensing out of those. Inhibition out of something very like quorum sensing. Building of commands still done by hand. 

$\Rightarrow$ Research quorum sensing in bacteria.

$\Rightarrow$ Could steal from neuronal wiring and growth for exploration and communication algorithms. 

% research questions (usually, 1-3 should suffice) and the reason for asking them
% the major approach(es) you will take (conceptual, theoretical, empirical and normative, as appropriate) and rationale
% significance of the research (in academic and, if appropriate, other fields)

%Highlight its originality and/or significance
%Explain how it adds to, develops (or challenges) existing literature in the field
%Persuade potential supervisors and/or funders of the importance of the work, and why you are the right person to undertake it

Computer is aware of meaning of gesture, locations of robots. 
Second assumption (global localization) may not hold. 
How can computer determine what programs or what parameters result in the completion of the task?

What are the tasks? Assume search and rescue domain, what are the tasks in USR? Search an area with good coverage. Report content of an area. Group at a location. Locate a specific resource. 

This could be situated at an intersection between planning and compiling, as the compilation might have to factor in elements of the known environment at the time of compilation. Since the actors are spatially situated, plans should incorporate spatial awareness.  

``Improving Efficiency in Mobile Robot Task Planning Through World Abstraction'' \cite{galindo2004improving} Hierarchical world representation and operating at multiple levels of that hierarchy to reduce dimensionality of the problem space. Successive refinement from broad goals to behavioral primitives. Primitives are available for swarm robotics, but overall actions take place over the swarm. How to actually accomplish the translation is a bit tricky. Even GPL and OSL work from a desired known final state to a behavior specification, so task specification would have to be based on a description of the final state of the system. 

Robots have some autonomy, similar to the existing work with extending to multi-actor teams, e.g. human/robot mixed team. 

Task allocation if robots are heterogeneous (battery life, terrain, etc. will make homogeneous into heterogeneous)

Proximity if robots are not (closest robot does task)

Difficulty arises when robots are not super-well localized

``Swarm robotics: a review from the swarm engineering perspective" \cite{brambilla2013swarm} Manuele Brambilla, Eliseo Ferrante, Mauro Birattari, Marco Dorigo. (2013) ``Swarm engineering is an emerging discipline that aims at defining systematic and well founded procedures for modeling, designing, realizing, verifying, validating, operating, and maintaining a swarm robotics system." Two taxonomies, one of design/analysis methods, one of collective behaviors. Classes automatic design of robot programs as either evolutionary or multi-robot reinforcement, no category for program synthesis at all. Trial-and-error approaches are a different area. Behavior-based with probabilistic FSMs, virtual physics (gas models and potential fields). Other stuff category: amorphous computation. 

Models tend to be macro or micro, not a lot of modeling across all levels of the swarm behavior. Paper has overviews of some of the mathematical models, not super-relevant to designing the program, but probably handy for simulation. Use of real robots rates a section, less than half of papers actually use real robots. People don't often state why the real robots were used (real sensor noise, real failures, things simulation elides). 

Collective behavior hierarchy covers a lot of the behavior primitives that papers on compositional models cover. 

HSI section just lists a bunch of interactions, gesture-based communication, robots observing operator and voting/consensing on what they operator signaled. Human placement of beacons or interaction via a central computer. 

$\Rightarrow$ Protoswarm language! There's a language for that! Find a paper, do it now! Unfortunately, this has UI of a programming language, so still not great for first responders who are not also coders. 


\subsection{GPS-denied Swarm Control}

Pretty much all of the papers I've read assumed that the robots had either no global positioning, or positioning based on a static beacon, which is effectively relative position to a robot that doesn't move. Tend to be more concerned with 

Determine how denial of a few usually-assumed-good properties of the system affects how the user tasks get compiled. 
Degrade and then remove inter-robot communication to see how system adapts when robots can't reach each other. 
Degrade and then remove global localization (GPS denial) to see what happens when robots cannot determine global absolute distance each other.

Robustness in a robot swarm is usually assumed to occur because robots can replace each other. 
This doesn't mean anything if the software can't deal with robots failing, or isn't itself robust in generating plans to cause the robots to behave robustly in the face of failure. 
A kid playing with legos can use a blue block if there's not a red one available, or two small ones if there's not a large one. 
Programs that claim to support a robust robot swarm should be at least that sturdy.

Performing actions relative to other robots easy when GPS is available. Without GPS, local ranging and RSSI. 

Absolute distance between robots

Absolute location within the world

Performing actions relative to other robots gets harder when everything is relative

GPS denial is trivial for hostile actors. Weak signal coming from space on a known frequency. Jammers are available cheap on the internet. Spoofing isn't hard, just slightly more expensive. 

GPS denial is also a fact of life for operating inside of buildings. Since urban disaster areas pretty much by definition include buildings, at least partial GPS denial is a valid assumption. 

Cell tower triangulation may not be available in wilderness rescue situations, widespread disasters

Relative distance between robots (RSSI, visual recognition of other robots). RSSI is prone to multi-path interference, only really valid over short ranges. IR and sonar have similar problems with reflections. 

Multi-robot SLAM, with shared map allowing placement of each robot in a shared environement. 
SLAM errors when bits of map appear to match up that shouldn't
Using "If you're there, you should be able to see me" to update each other's hypotheses about each other's locations.
Is this a done thing? It should be, it's pretty easy to make robots obvious to each other. 

Unreliable positioning leads to unreliable behavior. If robots in a chain are required to maintain a certain distance from each other, variable noise in the distance sensing will result in the chain becoming unstable as individual links hallucinate being too close or too far apart. 

Have to use e.g. foraging search to find each other, do area coverage, update map. Want to avoid redundancy in search, but that's hard to do without either marking the environment (collect whatever it is they are searching for, so later seekers detect that it is scarce and move on quickly, leave a repellent pheromone) or global mapping (mark areas of map as searched, move to unmapped areas). With high enough robot count and known area, diffusion/scattering will get a certain robot/area density that could count as ``coverage''. 

Ant colony optimization or similar approaches to swarming on a point (forming chains away from the point to collect others, following chains)

Stigmurgy is probably right out, at least in real search and rescue situations. Post-Katrina spraypainted markers for building search somewhat analogous to pheromone trails, marked environment to avoid duplication of work. Practically speaking, equipping a robot with the ability to mark the environment and the ability to perceive and make sense of marks is tricky.  

How to convey to user/world map that maybe their robots are not where they think they are (for all possible disambiguations of "they"). Circles around last-seen locations? Becomes counterintuitive to control something you can't see or determine if it's doing the right thing. How do robots develop their concept of doing the right thing, and how do they convey that along with uncertainty about it to the user?


\bibliography{swarm.bib}
\bibliographystyle{plain}

\end{document}

%Papers to read

%Cooperative interaction of walking human and distributed robot maintaining stability of swarm 

%Development of IR-based short-range communication techniques for swarm robot applications 

%The Wanda Robot and Its Development System for Swarm Algorithms

%Stability of swarm robot based on local forces of local swarms

%Swarm robot pattern formation using a morphogenetic multi-cellular based self-organizing algorithm 

%A particle-swarm-optimized fuzzy-neural network for voice-controlled robot systems 

%The I-SWARM project 


